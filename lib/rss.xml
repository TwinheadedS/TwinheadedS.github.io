<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Data Science]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Data Science</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 22 May 2024 14:04:27 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 22 May 2024 14:04:20 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[(6) Association Rules Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(6) Association Rules Overview" data-href="#(6) Association Rules Overview" href="\#(6)_Association_Rules_Overview" class="internal-link" target="_self" rel="noopener">(6) Association Rules Overview</a>

<br><a data-tooltip-position="top" aria-label="Rules" data-href="#Rules" href="\#Rules" class="internal-link" target="_self" rel="noopener">Rules</a>
<br><a data-tooltip-position="top" aria-label="Itemset" data-href="#Itemset" href="\#Itemset" class="internal-link" target="_self" rel="noopener">Itemset</a>
<br><a data-tooltip-position="top" aria-label="Apriori Algorithm" data-href="#Apriori Algorithm" href="\#Apriori_Algorithm" class="internal-link" target="_self" rel="noopener">Apriori Algorithm</a>
<br><a data-tooltip-position="top" aria-label="Evaluation of candidate rules" data-href="#Evaluation of candidate rules" href="\#Evaluation_of_candidate_rules" class="internal-link" target="_self" rel="noopener">Evaluation of candidate rules</a>

<br><a data-tooltip-position="top" aria-label="Confidence" data-href="#Confidence" href="\#Confidence" class="internal-link" target="_self" rel="noopener">Confidence</a>
<br><a data-tooltip-position="top" aria-label="Lift" data-href="#Lift" href="\#Lift" class="internal-link" target="_self" rel="noopener">Lift</a>
<br><a data-tooltip-position="top" aria-label="Leverage" data-href="#Leverage" href="\#Leverage" class="internal-link" target="_self" rel="noopener">Leverage</a>
<br><a data-tooltip-position="top" aria-label="Summary" data-href="#Summary" href="\#Summary" class="internal-link" target="_self" rel="noopener">Summary</a>


<br><a data-tooltip-position="top" aria-label="Applications of Association Rules" data-href="#Applications of Association Rules" href="\#Applications_of_Association_Rules" class="internal-link" target="_self" rel="noopener">Applications of Association Rules</a>
<br><a data-tooltip-position="top" aria-label="Validation and Testing" data-href="#Validation and Testing" href="\#Validation_and_Testing" class="internal-link" target="_self" rel="noopener">Validation and Testing</a>
<br><a data-tooltip-position="top" aria-label="Diagnostic" data-href="#Diagnostic" href="\#Diagnostic" class="internal-link" target="_self" rel="noopener">Diagnostic</a>


<br><br>Association rules are a fundamental concept in data mining and machine learning<br>
Can be used in recommendation systems, used to discover the potential connection between one product to another<br>
<br>It is an unsupervised learning method
<br>A descriptive, not predictive method
<br>Used to discover interesting relationships in a large dataset
<br>The disclosed relationships are represented as rules or frequent itemsets
<br>Commonly used for mining transactions in database
<br>e.g. Market basket<br>
<img alt="Association rules example image (1).png" src="\lib\media\association-rules-example-image-(1).png"><br>
These are some questions that association rules can help answer :<br>
<br>Which products tends to be purchased together?
<br>Of those customers who are similar to this person, what products do they tend to buy?
<br>Of those customers who have purchased this product, what other similar products do they tend to view or purchase?
<br>e.g. A transaction of three stacks of receipts<br>
The rules suggest that when cereal is purchased 90% of the time milk is purchased, when bread is purchased, 40% of the time milk is purchased also; when milk is purchased, 23% of the time cereal is also purchased<br><br>Each rule is in the form of X -&gt; Y , where X and Y are non-overlapping itemsets<br>
e.g. {Milk, Diaper} -&gt; {Beer}<br>Steps of Generating associations rules :<br>
<br>Find frequent itemsets whose occurrences exceed a predefined minimum support threshold
<br>Derive association rules from those frequent itemsets (With the constraints of minimum confidence threshold)
<br>Support<br>
No. of transactions that contain both X and Y out of total no of transactions<br>
e.g.<br>
Total number of transactions: 7<br>
Transactions with both bread and milk: 3 (Transactions 1, 3, 6)<br>
So the support for {bread, milk} is 3/7 = 0.433/7 = 0.43 or approximately 43%<br>Confidence<br>
No, of transactions that contain both X and Y out of total no, of transactions that contain X<br>
e.g.<br>
Total transactions containing bread: 5 (Transactions 1, 2, 3, 5, 6)<br>
Transactions containing both bread and milk: 3 (Transactions 1, 3, 6)<br>
The confidence for {bread}→{milk} is 3/5 = 0.63/5 = 0.6 or 60%.<br><br>A collection of items or individuals entities that contain some kind of relationship<br>An itemset containing k items is called a k-itemset = {item 1, item 2, ..., item k}<br>
e.g. a set of retail items purchased together in one transaction, a set of hyperlinks clicked on by user in a single session<br><br>One of the most fundamental algorithms for generating association rules<br>
One major component of Apriori is support<br>
<br>Given an itemset L, the support of L is the percentage of transactions that contain L
<br>If 80% of all transactions contain itemset {bread}, then the support of {bread} is 0,8
<br>if 60% of all transactions contain itemset {bread, butter}, then the support of {bread, butter} is 0,6
<br>Apriori Property (Downward closure property)<br>
If an item is considered frequent, then any subset of the frequent itemset must also be frequent<br>
<a data-href="Creating Frequent Sets Example" href="\cos10022-data-science-principles\(6)-association-rules\creating-frequent-sets-example.html" class="internal-link" target="_self" rel="noopener">Creating Frequent Sets Example</a><br>Main steps of iteration are (C as a candidate itemset of size k and L as a frequent itemset of size k):<br>
<br>Find frequent itemset  (Starting from L)
<br>Join Step : C is generated by joining  with itself (cartesian product  x  )
<br>Prune step : any (K-1) size itemset that is not frequent cannot be a subset of a frequent k size itemset, hence should be removed from C
<br>Frequent set L has been achieved
<br><img alt="Apriori Algorithm Pseudo Code Image (1).png" src="\lib\media\apriori-algorithm-pseudo-code-image-(1).png"><br><br><br>The process of creating association rules is two-staged<br>
First, a set of candidate rule based on frequent itemsets is generated<br>Second, The appropriateness of theses candidate rules are evaluated using :<br>
<br>Confidence
<br>Lift
<br>Leverage
<br><br>Confidence is the measures of certainty or trustworthiness associated with each discovered rule<br>
Mathematically, the percent of transactions that contain both X and Y out of all the transactions that contain X<br>
<br>A relationship may be thought of as interesting when the algorithm identifies the relationship with a measure of confidence greater than or equal to the predefined threshold (the minimum confidence)<br>Confidence has several flaw :<br>
<br>Given a rule X -&gt; Y, confidence considers only the antecedent (X) and the co-occurrence of X and Y
<br>Cannot tell if a rule contains true implication of the relationship or if the rule is purely coincidental
<br><br>Lift measures how many times more often X and Y occur together than expected if they are statistically independent of each other<br>
A measure of how X and Y are really related rather than coincidentally happening together
<br>Lift = 1, then X and Y are statistically independent<br>
Lift &gt; 1, indicates that the degree of usefulness of the rules (A larger value of lift suggests a greater strength of the association between X and Y)<br>e.g. <img alt="Lift example in Association Rules (1).png" src="\lib\media\lift-example-in-association-rules-(1).png"><br><br>Measures the difference in the probability of X and Y appearing together in the dataset compared to what would be expected if X and Y were statistically independent of each other<br><br>Leverage = 0, means that X and Y are statistically independent<br>
Leverage &gt; 0, indicates that the degree of relationship between X and Y (Larger value indicates stronger relationship)<br>e.g. <img alt="Leverage example in association rules (1).png" src="\lib\media\leverage-example-in-association-rules-(1).png"><br><br>
<br>Confidence is able to identify trustworthy rules, but it cannot tell whether a rule is a coincidental
<br>Measures such as lift and leverage not only ensure interesting rules are identified but also filter out the coincidental rules
<br>Support, confidence, lift and leverage ensures the discovery of interesting and strong rules from the sample dataset
<br><br><br>The term market basket analysis refers to a specific implementation of association rules<br>
<br>For better merchandising - Products to include / exclude from inventory each month
<br>Placements of products
<br>Cross-selling, e.g. Bundling items with high correlation together
<br>Promotional programs - Multiple product purchase incentives managed through a loyalty card program
<br>Input : The simple point-of-sale transaction data<br>
Output : Most frequent affinities among items<br>
Example : According to the transaction data<br>
"Customer who bought a laptop computer and a virus protection software, also bought extended service plan 70% of the time"<br>How to use the pattern/knowledge in a practical terms?<br>
<br>Put the items next to each other for ease of finding
<br>Promote the items as a package (Do not put one on sale if the other(s) are on sale)
<br>Place items far apart from each other so that the customer must walk the aisles to search for it, and by doing so potentially seeing and buying other items
<br>Recommender systems - Amazon, Netflix<br>
<br>Clickstream analysis from web usage log files
<br>Website visitors to page X click on links A,B, C more than on links D, E, F
<br>In Medicine<br>
<br>Relationships between symptoms and illnesses
<br>Diagnosis and patient characteristics and treatments (Usually to be used in medical DSS)
<br>Genes and their functions (Usually to be use in genomics projects)
<br><br>The frequent and high confidence itemsets are found by pre-specified minimum support and minimum confidence levels<br>Measures like Lift or Leverage then ensure that interesting rules are identified rather than coincidental ones<br>However, some of the remaining rules may be considered subjectively uninteresting because the don't yield unexpected profitable actions<br>
e.g. Rules like {paper} -&gt; {pencil} are not interesting / meaningful<br>
Domain Knowledge is important here!
<br><br>
<br>Although the Apriori algorithm is easy to understand and implement, some of the rules generated are practically useless
<br>Additionally, some of the rules may be generated due to coincidental relationships between the variables
<br>Measures like confidence, lift and leverage should be used along with human insights to address this problem
<br>Algorithm requires a scan of the entire database to obtain the result. Accordingly, as the database grows, it takes more time to compute in each run
<br>Approaches to improve Apriori's efficiency<br>
<br>
Partitioning<br>
Any itemset that is potentially frequent in a transaction database must be frequent in at least one of the partitions of the transaction database

<br>
Sampling<br>
This extracts a subset of the data with a lower support threshold and uses the subset to perform association rule mining

<br>
Transaction Reduction<br>
A transaction that does not contain frequent k-itemsets is useless in subsequent scans and therefore can be ignored

<br>
Hash-based itemset counting<br>
If the corresponding hashing bucket count of a k-itemset is below a certain threshold, the k-itemset cannot be frequent

<br>
Dynamic itemset counting<br>
Only add new candidate itemsets when all of their subsets are estimated to be frequent

]]></description><link>cos10022-data-science-principles\(6)-association-rules\(6)-association-rules.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(6) Association Rules/(6) Association Rules.md</guid><pubDate>Wed, 22 May 2024 10:06:30 GMT</pubDate><enclosure url="lib\media\association-rules-example-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\association-rules-example-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Creating Frequent Sets Example]]></title><description><![CDATA[ 
 <br><br>Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore (k+1) - itemsets<br>When using top to bottom method, we check occurrences from a single item denoted by L1, two items denoted by L2 and so forth, until no more frequent k-itemsets can be found<br>
The finding of each Lk requires one full scan of the database! (Which going to make searching a while)
<br>e.g. Transactions<br><br>We define the minimum support 3/5 = 60% which we want our support must be above 3<br>L1 (1-itemsets)<br><br>L2 (2-itemsets) (No need to generate candidates involving Coke or Eggs)<br><br>L3 (3-itemsets) (We don't need to go down further as they are already in a minimal support)<br><br>Another example :<br>
<img alt="Apriori Algorithm Example (2).png" src="\lib\media\apriori-algorithm-example-(2).png">]]></description><link>cos10022-data-science-principles\(6)-association-rules\creating-frequent-sets-example.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(6) Association Rules/Creating Frequent Sets Example.md</guid><pubDate>Mon, 29 Apr 2024 04:34:21 GMT</pubDate><enclosure url="lib\media\apriori-algorithm-example-(2).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\apriori-algorithm-example-(2).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(7) Data Analytics Lifecycle Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(7) Data Analytics Lifecycle Overview" data-href="#(7) Data Analytics Lifecycle Overview" href="\#(7)_Data_Analytics_Lifecycle_Overview" class="internal-link" target="_self" rel="noopener">(7) Data Analytics Lifecycle Overview</a>

<br><a data-tooltip-position="top" aria-label="HiPPO Effect" data-href="#HiPPO Effect" href="\#HiPPO_Effect" class="internal-link" target="_self" rel="noopener">HiPPO Effect</a>
<br><a data-tooltip-position="top" aria-label="Phase 1 - Discovery" data-href="#Phase 1 - Discovery" href="\#Phase_1_-_Discovery" class="internal-link" target="_self" rel="noopener">Phase 1 - Discovery</a>

<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>


<br><a data-tooltip-position="top" aria-label="Phase 2 - Data Preparation" data-href="#Phase 2 - Data Preparation" href="\#Phase_2_-_Data_Preparation" class="internal-link" target="_self" rel="noopener">Phase 2 - Data Preparation</a>

<br><a data-tooltip-position="top" aria-label="When to use ETL or ELT?" data-href="#When to use ETL or ELT?" href="\#When_to_use_ETL_or_ELT" class="internal-link" target="_self" rel="noopener">When to use ETL or ELT?</a>
<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>


<br><a data-tooltip-position="top" aria-label="Phase 3 - Model Planning" data-href="#Phase 3 - Model Planning" href="\#Phase_3_-_Model_Planning" class="internal-link" target="_self" rel="noopener">Phase 3 - Model Planning</a>

<br><a data-tooltip-position="top" aria-label="Model Selection" data-href="#Model Selection" href="\#Model_Selection" class="internal-link" target="_self" rel="noopener">Model Selection</a>
<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>


<br><a data-tooltip-position="top" aria-label="Phase 4 - Model Building" data-href="#Phase 4 - Model Building" href="\#Phase_4_-_Model_Building" class="internal-link" target="_self" rel="noopener">Phase 4 - Model Building</a>

<br><a data-tooltip-position="top" aria-label="Workflow" data-href="#Workflow" href="\#Workflow" class="internal-link" target="_self" rel="noopener">Workflow</a>
<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>


<br><a data-tooltip-position="top" aria-label="Phase 5 - Communicate Results" data-href="#Phase 5 - Communicate Results" href="\#Phase_5_-_Communicate_Results" class="internal-link" target="_self" rel="noopener">Phase 5 - Communicate Results</a>

<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>


<br><a data-tooltip-position="top" aria-label="Phase 6 - Operationalise" data-href="#Phase 6 - Operationalise" href="\#Phase_6_-_Operationalise" class="internal-link" target="_self" rel="noopener">Phase 6 - Operationalise</a>

<br><a data-tooltip-position="top" aria-label="Key Activities" data-href="#Key Activities" href="\#Key_Activities" class="internal-link" target="_self" rel="noopener">Key Activities</a>
<br><a data-tooltip-position="top" aria-label="Discovery case study" data-href="#Discovery case study" href="\#Discovery_case_study" class="internal-link" target="_self" rel="noopener">Discovery case study</a>




<br><br>The real world is chaotic and so are its problems and data, that is why we need an order and sense-making in order to be able to measure the measures of success<br><br>Highest Paid Person's Opinion (HiPPO) effect states that the authority figure's suggestions are interpreted as the final truth and promptly implemented even if the findings from the data are contrary<br>
Let the data speaks for you but not the Human Bias
<br><br>Data science team learns the business domain and assesses the resources available to support the project in terms of people, technology, time and data<br>Activities that are important to do here are to frame the business problem as an analytics challenge and formulating initial hypotheses (IHs) to test and begin learning the data<br><br>
<br>Learning the business domain
<br>Assessing the resources available to support the project
<br>Framing the problem<br>
Establishing failure criteria, identify what needs to be achieved in business terms
<br>Identify key stakeholders<br>
This activity asks who will benefit from the project if the project succeeds?
<br>Interviewing the analytics sponsor<br>
Who should be on the team based on the problem to solve, defining the boundaries condition
<br>Developing Initial Hypotheses (HIs)
<br>Identifying potential data sources<br>
Here we also decide the sort of data infrastructure needed for this type of probem
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/03/phase-1-innovation-analytics.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/03/phase-1-innovation-analytics.html" target="_blank">Information Playground: Phase 1 Innovation Analytics (typepad.com)</a><br><br><br><br>Phase 2 requires the presence of an analytics sandbox, here they perform data cleaning as not all data will be in the format that we want. The team performs ETLT (Extraction, Transform and Load)to get the data into the sandbox<br><br>ETL (Extraction, Transform, Load) is to merge different data in different databases into one database. It also clean dirty data that have different formats.<br>
<img alt="ETL Example In the data analytics lifecycle (1).png" src="\lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png"><br>ELT (Extraction, Load, Transform) is the same to merge different data in different databases into one database but with different approach<br>
<img alt="ELT Example in the Data analytics Lifecycle (1).png" src="\lib\media\elt-example-in-the-data-analytics-lifecycle-(1).png"><br><br>
<br>Preparing the analytic sandbox<br>
We need to create an independence space for experimental trial to avoid permanent damage to the data. We usually create 5x or 10x greater space comparing to the original dataset for the data to grow
<br>Performing ETLT<br>
Select proper tools to move that data into the sandbox e.g. Hadoop, MapReduce, Twitter API
<br>Learning about the data<br>
Look if something is wrong in the data, typos, errors, inconsistent data formats. Also create dataset inventory (Creating a data checklist, which data can do what)
<br>Data conditioning<br>
What are the data sources? What are the target fields, columns and attributes? How clean is the data? How consistent are the contents and fields?
<br>Survey and Visualize<br>
Does the data represent the population of interest? Does the data distribution stay consistent over all the data?
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/03/phase-2-innovation-analytics-data-preparation-1.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/03/phase-2-innovation-analytics-data-preparation-1.html" target="_blank">Information Playground: Phase 2 Innovation Analytics: Data Preparation (typepad.com)</a><br><br><br><br>The data science team determines the methods, techniques and workflow. The team also explores the data to learn about the relationships between variables to select the most suitable models<br><br>This activity aims at choosing an analytical technique (a model) based on the end goal of the project<br>What is a model?<br>
A thing that we can describe happening in the natural world (an attempt to understand reality)<br>The popular model that we are using nowadays are in the machine learning algorithms field (most of it is based on statistics)<br>There are three different approaches to selecting machine learning models :<br>
<br>Supervised Learning
<br>Unsupervised Learning
<br>Semi-supervised learning
<br><br>
<br>Data exploration and variable selection<br>
Understand the variable. Stakeholders and domain experts may have pre-existing yet flawed assumptions about the data, Hence, the data science team's role is to objectively question these assumptions and correct and bias. Capture the most essential predictors!
<br>Model Selection<br>
Common tools to be used = R, SQL Analysis Service, Microsoft SAS / Access
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/05/phase-3-innovation-analytics-.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/05/phase-3-innovation-analytics-.html" target="_blank">Information Playground: Phase 3 Innovation Analytics: Model Planning (typepad.com)</a><br><br><br><br>The team develops datasets from testing, training and production purposes. The team builds and executes models based on the work done in the model planning phase<br>We do not always code an entirely new analytics model from scratch. Rather we select and experiment with various models, where applicable, fine tuning their parameters<br>Documentation is important here, it is vital to record the results and logic of the model. We must record any operating assumptions made concerning the data<br><br><img alt="Model Building Workflow (1).png" src="\lib\media\model-building-workflow-(1).png"><br><br>
<br>Develop an analytical model, fit in on the training data and evaluate its performance on the Data<br>
Training data : Used to discover a predictive relationship<br>
Test data : Used to assess the strength and utility of a predictive relationship<br>
Test and training data are independent of each other (Non-overlapping)
<br>Several key important questions to ask are :<br>
<br>Does the model output / behaviour  make sense to the domain experts?
<br>Is a different form of the model required to address the business problem?
<br>Commercial tools used in this phase :<br>
<br>SAS Enterprise Miner
<br>IBM SPSS Modeler
<br>Matlab
<br>Chorus 6
<br>Open Source tools used in this phase :<br>
<br>R (R Commands can be executed in database)
<br>Octave (Computational modelling with some functionalities of Matlab)
<br>Weka (Rich Java API)
<br>Python (Rich machine and data visualisation libraries)
<br>MADlib
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/06/finding-boundary-spanners.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/06/finding-boundary-spanners.html" target="_blank">Information Playground: Phase 4 Innovation Analytics: Finding Boundary Spanners (typepad.com)</a><br><br><br><br>The team determines if the results of the project are a success or a failure based on the criteria developed in Phase 1<br><br>
<br>Compare outcomes to judge whether success or failure<br>
Take into account caveats, assumptions and any limitation of the results
<br>Determine whether the project succeeded or failed in its objectives<br>
Failure should not be considered as a true failure. Rather, it should be viewed as a failure of the data to accept or reject a given hypothesis
<br>Communicate model that addresses the analytical challenge in the most appropriate way<br>
Select the three most significant ones that can be shared and reflect on the implications of these findings and measure the business value
<br>Reflect on the project, the encountered obstacles and what can be improved in the future<br>
Make recommendations for future work or improvements to the existing processes
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/06/phase-5-innovation-analytics-global-knowledge-flight-patterns.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/06/phase-5-innovation-analytics-global-knowledge-flight-patterns.html" target="_blank">Information Playground: Phase 5 Innovation Analytics: Global Knowledge Flight Patterns (typepad.com)</a><br><br><br><br>The team delivers final reports, briefings, code and technical documents. Team may run a pilot project to implement the models in a production environment<br><br>
<br>Set up a pilot project to deploy the work in a controlled way before broadening the work to a full enterprise scale<br>
The risk can be better managed this way and in a way adjustments can still be made before a full deployment
<br>Manage the deployment scope<br>
Consider running a model in a production environment for a discrete set of products. Minimising risks, while allowing to fine-tune the model
<br>Create mechanism for performing ongoing monitoring of model accuracy and if accuracy degrades, find ways to retrain the model
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/07/phase-6-innovation-analytics-operationalize.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/07/phase-6-innovation-analytics-operationalize.html" target="_blank">Information Playground: Phase 6 Innovation Analytics: Operationalize (typepad.com)</a><br><br>]]></description><link>cos10022-data-science-principles\(7)-data-analytics-lifecycle\(7)-data-analytics-lifecycle.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(7) Data Analytics Lifecycle/(7) Data Analytics Lifecycle.md</guid><pubDate>Wed, 22 May 2024 10:15:54 GMT</pubDate><enclosure url="lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Analytics Lifecycle Canvas]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br>Do I have enough information to draft an analytic plan?<br>Do I have enough "good" data to start building the model?<br>Do I have a good idea about the type of model to try? Can I refine my analytical plan?<br>Is the model robust enough? Have we failed enough?]]></description><link>cos10022-data-science-principles\(7)-data-analytics-lifecycle\data-analytics-lifecycle-canvas.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(7) Data Analytics Lifecycle/Data Analytics Lifecycle Canvas.canvas</guid><pubDate>Thu, 02 May 2024 02:44:28 GMT</pubDate></item><item><title><![CDATA[Problem Formulation Overview]]></title><description><![CDATA[ 
 <br><br>Using proper techniques to solve the problem<br>]]></description><link>cos10022-data-science-principles\(7)-data-analytics-lifecycle\problem-formulation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(7) Data Analytics Lifecycle/Problem Formulation.md</guid><pubDate>Wed, 22 May 2024 10:27:45 GMT</pubDate></item><item><title><![CDATA[Data Preparation Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="Data Preparation Overview" data-href="#Data Preparation Overview" href="\#Data_Preparation_Overview" class="internal-link" target="_self" rel="noopener">Data Preparation Overview</a>

<br><a data-tooltip-position="top" aria-label="Data Cleaning" data-href="#Data Cleaning" href="\#Data_Cleaning" class="internal-link" target="_self" rel="noopener">Data Cleaning</a>

<br><a data-tooltip-position="top" aria-label="Incomplete Data" data-href="#Incomplete Data" href="\#Incomplete_Data" class="internal-link" target="_self" rel="noopener">Incomplete Data</a>
<br><a data-tooltip-position="top" aria-label="Noisy Data" data-href="#Noisy Data" href="\#Noisy_Data" class="internal-link" target="_self" rel="noopener">Noisy Data</a>
<br><a data-tooltip-position="top" aria-label="Data Cleaning as a Process" data-href="#Data Cleaning as a Process" href="\#Data_Cleaning_as_a_Process" class="internal-link" target="_self" rel="noopener">Data Cleaning as a Process</a>


<br><a data-tooltip-position="top" aria-label="Data Integration" data-href="#Data Integration" href="\#Data_Integration" class="internal-link" target="_self" rel="noopener">Data Integration</a>
<br><a data-tooltip-position="top" aria-label="Data Reduction" data-href="#Data Reduction" href="\#Data_Reduction" class="internal-link" target="_self" rel="noopener">Data Reduction</a>
<br><a data-tooltip-position="top" aria-label="Data Transformation" data-href="#Data Transformation" href="\#Data_Transformation" class="internal-link" target="_self" rel="noopener">Data Transformation</a>

<br><a data-tooltip-position="top" aria-label="Normalization" data-href="#Normalization" href="\#Normalization" class="internal-link" target="_self" rel="noopener">Normalization</a>
<br><a data-tooltip-position="top" aria-label="Discretization" data-href="#Discretization" href="\#Discretization" class="internal-link" target="_self" rel="noopener">Discretization</a>
<br><a data-tooltip-position="top" aria-label="Decision Tree Analysis" data-href="#Decision Tree Analysis" href="\#Decision_Tree_Analysis" class="internal-link" target="_self" rel="noopener">Decision Tree Analysis</a>
<br><a data-tooltip-position="top" aria-label="Correlation Analysis" data-href="#Correlation Analysis" href="\#Correlation_Analysis" class="internal-link" target="_self" rel="noopener">Correlation Analysis</a>
<br><a data-tooltip-position="top" aria-label="Concept Hierarchy Generation" data-href="#Concept Hierarchy Generation" href="\#Concept_Hierarchy_Generation" class="internal-link" target="_self" rel="noopener">Concept Hierarchy Generation</a>




<br><br>Given the presence of an analytics sandbox, the data science team-work with data and perform analytics for the duration of the project<br>Why is data preparation important?<br>
Well-prepared data set will saves us from debugging and additional works in the project. An organized data will be easily to be tracked and accessed<br>Data preparation must contain data quality :<br>
<br>Accuracy
<br>Completeness
<br>Consistency
<br>Timeliness
<br>Believability
<br>Interpretability
<br>Major tasks in data preparation :<br>
<br><a data-href="#Data Cleaning" href="\#Data_Cleaning" class="internal-link" target="_self" rel="noopener">Data Cleaning</a>
<br><a data-href="#Data Integration" href="\#Data_Integration" class="internal-link" target="_self" rel="noopener">Data Integration</a>
<br><a data-href="#Data Transformation" href="\#Data_Transformation" class="internal-link" target="_self" rel="noopener">Data Transformation</a>
<br><a data-href="#Data Reduction" href="\#Data_Reduction" class="internal-link" target="_self" rel="noopener">Data Reduction</a>
<br><br><br>Real-world data is dirty<br>
Here we fill in missing values, smooth noisy data, identify outliers and resolve inconsistency<br><br>This can occur because a number of reasons :<br>
<br>Attributes of interest may not always be available
<br>Not recorded due to equipment malfunctions
<br>Data that may have been deleted
<br>Missing data, particularly for tuples with missing values for some attributes, may need to be inferred<br>6 Ways of handling incomplete data :<br>
<br>Ignore the tuple
<br>Fill in the missing values with side information
<br>Use a global constant to fill in the missing values (N/A)
<br>Use a measure of central tendency for the attribute (e.g. the mean or medium)
<br>Use the attribute mean or medium for all the samples belonging to the same class as the give tuples
<br>Use the most probable value to fill in the missing value<br>
This may be determined with regression, interference-based tools using bayesian formalism or decision tree induction<br>
e.g. using the other similar attributes, we may construct a decision tree
<br><br>Noisy data is a random error or a variance in a measured variable<br>There are several ways to calibrate the data :<br>
<br>Binning (Data Smoothing)<br>
Smoothing the data by consulting its "Neighborhood", the values around it<br>
The sorted values are distributed into a number of "buckets" or "bins"<br>
More bins = higher resolution
<br>Regression<br>
A technique that conforms data values to a function, finding the "best" line to fit two attributes so that one attribute can be used to predict the others
<br>Sliding Window (Moving Average) (Convolution)<br>
Using the neighbourhood data to find the average, this requires to slide through the whole data in sequence
<br>Outlier Analysis<br>
Outliers may also be detected by clustering, where similar values are organized into groups or clusters
<br>Incorrect attributes value may be due to : <br>
<br>Faulty equipment
<br>Data entry problems
<br>Other data problems that require data cleaning :<br>
<br>Duplicate Records
<br>Incomplete Data
<br>Inconsistent Data
<br><br>The first step in data cleaning as a process is discrepancy detection<br>
Discrepancy can be caused by :<br>
<br>Human errors in data entry
<br>Deliberate errors
<br>Data decay (Outdated data)
<br>System errors
<br>How to detect data discrepancy?<br>
<br>Metadata<br>
What are the acceptable values for each attribute? 
<br>Check uniqueness rule, consecutive rule and null rule<br>
Use special characters to replace missing values
<br>Use commercial tool (e.g. Microsoft BI)<br>
Data Scrubbing (Use domain knowledge)<br>
Data Auditing (Correlation and Clustering to find outliers)
<br>Data migration and integration<br>
ETL tools that can specify transformations through a graphical user interface
<br><br><br>Data integration combines data from multiple sources into a coherent store, as in data warehousing<br>
We need to also solve the data value conflicts (American metrics vs British metrics)<br>How can equivalent real-world entities from multiple data sources be matched up? (Entity Identification Problem)<br>
e.g. Customer Id in one database and customer number in a another database<br>Redundant data often occurs between databases<br>
<br>Object identification (The same attribute may have have different names in different databases)
<br>Derivable Data (One attribute may be a "derived" attribute in another table)
<br>Redundant data can also be detected by correlation analysis<br>
Categorical data = X <a data-href="(Chi-Square) test" href="\cos10022-data-science-principles\(8)-data-preparation\(chi-square)-test.html" class="internal-link" target="_self" rel="noopener">(Chi-Square) test</a><br>
Numerical data = <a data-href="Correlation coefficient" href="\cos10022-data-science-principles\(8)-data-preparation\correlation-coefficient.html" class="internal-link" target="_self" rel="noopener">Correlation coefficient</a>, Covariance<br><br><br>Data reduction is obtaining a reduced representation of the data set that is much smaller in volume but yet produce the same analytical results<br>A data warehouse may store terabytes of data. As we do complex data analysis / mining resulting in a very long time to run on the complete data set<br>Several data reduction strategies :<br>
<br><a data-href="Data cube aggregation" href="\cos10022-data-science-principles\(8)-data-preparation\data-cube-aggregation.html" class="internal-link" target="_self" rel="noopener">Data cube aggregation</a>
<br><a data-href="Attribute subset selection" href="\cos10022-data-science-principles\(8)-data-preparation\attribute-subset-selection.html" class="internal-link" target="_self" rel="noopener">Attribute subset selection</a>
<br><a data-href="Dimensionality reduction" href="\cos10022-data-science-principles\(8)-data-preparation\dimensionality-reduction.html" class="internal-link" target="_self" rel="noopener">Dimensionality reduction</a>
<br><a data-href="Numerosity reduction" href="\cos10022-data-science-principles\(8)-data-preparation\numerosity-reduction.html" class="internal-link" target="_self" rel="noopener">Numerosity reduction</a>
<br>Discretization and concept hierarchy generation
<br><br><br>There are several ways to transform the data :<br>
<br>Smoothing
<br>Attribute / Feature construction
<br>Aggregation
<br>Nomalization
<br>Discretization
<br>Concept Hierarchy generation for nominal data
<br><br>Normalization are useful for classification algorithms :<br>
<br>When using Neural network backpropagation algorithm for classification mining, this helps to speed up the learning phase
<br>When using distance-based method for clustering, this helps prevent attributes with initially large range from outweighing attributes with initially smaller ranges
<br>How to do Normalization?<br>
<img alt="How to do normalization (1).png" src="\lib\media\how-to-do-normalization-(1).png"><br>
<br>Min - Max<br>
Causes us to transform the data into between [0,1]
<br>Z-score<br>
It is useful when the actual min and max of attribute are unknown
<br>Decimal scaling (Least used)<br>
Usually used when we convert different measurements, transforming data from [-1,1]
<br><br>It transforms numeric data by mapping values to interval or concept label<br>Discretization techniques :<br>
<br>Binning
<br>Histogram analysis
<br>Cluster analysis
<br>Decision tree analysis
<br>Correlation analysis
<br>For nominal data :<br>
<br>Concept hierarchy
<br><br>It uses a top-down splitting approach<br>
Supervised = makes use of the class label<br>
It also uses entropy to determine split point<br><img alt="Decision Tree Analysis (1).png" src="\lib\media\decision-tree-analysis-(1).png"><br><br>It uses a bottom-up merge approach<br>
supervised = make use of the class label<br>ChiMerge = find the best neighboring intervals (Those having similar distributions of classes)<br><br>Nominal attributes have a finite(but possibly large) number of distinct values, with no ordering among the values<br>
It reduce the data by collecting and replacing low level concepts such as numeric values for age by higher level concepts (youth, adult, senior)<br>
<img alt="Hierarchy Concept Generation Example (1).png" src="\lib\media\hierarchy-concept-generation-example-(1).png">]]></description><link>cos10022-data-science-principles\(8)-data-preparation\(8)-data-preparation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/(8) Data Preparation.md</guid><pubDate>Wed, 22 May 2024 10:53:39 GMT</pubDate><enclosure url="lib\media\how-to-do-normalization-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\how-to-do-normalization-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Chi-Square test]]></title><description><![CDATA[ 
 <br><br>Chi-squared (X) test for categorical data<br>
<img alt="Chi-squared formula Image (1).png" src="\lib\media\chi-squared-formula-image-(1).png"><br>
<img alt="Chi squared example (1).png" src="\lib\media\chi-squared-example-(1).png">Hypothesis for the example : Gender and preferred reading are independent<br>
<img alt="Chi squared test example Image (2).png" src="\lib\media\chi-squared-test-example-image-(2).png">]]></description><link>cos10022-data-science-principles\(8)-data-preparation\(chi-square)-test.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/(Chi-Square) test.md</guid><pubDate>Sat, 04 May 2024 12:07:34 GMT</pubDate><enclosure url="lib\media\chi-squared-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\chi-squared-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Attribute subset selection]]></title><description><![CDATA[ 
 <br><br>Attribute subset selection reduces the data set size by removing irrelevant or redundant attributes<br>Heuristic methods that explore reduced search space to find "good" subset of the original attributes :<br>
<br><a data-href="#Stepwise forward selection" href="\#Stepwise_forward_selection" class="internal-link" target="_self" rel="noopener">Stepwise forward selection</a>
<br><a data-href="#Stepwise backward elimination" href="\#Stepwise_backward_elimination" class="internal-link" target="_self" rel="noopener">Stepwise backward elimination</a>
<br><a data-href="#Combination of forward and backward" href="\#Combination_of_forward_and_backward" class="internal-link" target="_self" rel="noopener">Combination of forward and backward</a>
<br>Decision tree induction
<br>The attributes are typically determined using tests of statistical significance, which assumes that the attributes are independent of one another<br>
Other attribute evaluation such as information gain is used in building decision trees for classification<br><br>
<br>Start with an empty set of attributes
<br>Determine the best of the original attributes and add it to the reduces set
<br>At each step, add the best of the remaining original attributes to the reduced set
<br><br>
<br>Start with the full set of attributes
<br>At each step, remove the worst attribute remaining in the set
<br><br>
<br>Start with an empty set of attributes
<br>At each step, add the best attribute to the reduced set and removes the worst from among the remaining attributes<br>
<img alt="Forward and backward elimination and selection Image (1).png" src="\lib\media\forward-and-backward-elimination-and-selection-image-(1).png">
<br><br>Decision tree induction constructs a flowchart-like structure where each internal (nonleaf) node denotes a test on an attribute, each branch corresponds to an outcome of the test, and each external (leaf) node denotes a class prediction.<br>
Decision tree induction is also almost always a binary tree (Two option tree)<br>
<img alt="Decision tree induction example (1).png" src="\lib\media\decision-tree-induction-example-(1).png"><br>
All attributes that do not appear in the tree are assumed to be irrelevant
]]></description><link>cos10022-data-science-principles\(8)-data-preparation\attribute-subset-selection.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Attribute subset selection.md</guid><pubDate>Wed, 15 May 2024 22:28:12 GMT</pubDate><enclosure url="lib\media\forward-and-backward-elimination-and-selection-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\forward-and-backward-elimination-and-selection-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Correlation coefficient]]></title><description><![CDATA[ 
 <br><br>Correlation coefficient (r) for numerical data (Pearson's product moment coefficient)<br>
<img alt="Correlation Coefficient Formula Image (1).png" src="\lib\media\correlation-coefficient-formula-image-(1).png"><br>We can visually evaluate correlation using scatter plots<br>
scatterplots shows the correlation coefficient from -1 to 1<br>
r = 1.0 = A perfect positive relationship<br>
r = 0 = No relationship<br>
r = -1.0 = A perfect negative relationship]]></description><link>cos10022-data-science-principles\(8)-data-preparation\correlation-coefficient.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Correlation coefficient.md</guid><pubDate>Sat, 04 May 2024 11:49:10 GMT</pubDate><enclosure url="lib\media\correlation-coefficient-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\correlation-coefficient-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Covariance]]></title><description><![CDATA[ 
 <br><br>Covariance (Cov) for numerical data<br>
<img alt="Covariance Formula Image (1).png" src="\lib\media\covariance-formula-image-(1).png"><br>
We can visually evaluate covariance between two variables using scatter plot<br>
Cov(A, B) &lt; 0 : A and B tend to move in the opposite direction<br>
Cov(A, B) &gt; 0 : A and B tend to move in the same direction<br>
Cov(A, B) = 0 : A and B are independent]]></description><link>cos10022-data-science-principles\(8)-data-preparation\covariance.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Covariance.md</guid><pubDate>Sat, 04 May 2024 11:52:52 GMT</pubDate><enclosure url="lib\media\covariance-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\covariance-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Cube Aggregation]]></title><description><![CDATA[ 
 <br><br>Aggregating several 2D table into a 3D table<br>
The resulting dataset is smaller in volume without loss of information necessary for the analysis task<br><img alt="Data Cube Aggregation Image (1).png" src="\lib\media\data-cube-aggregation-image-(1).png"><br>
e.g. An electronic company sales per quarter from year 2002 to 2004<br>
Data cubes store multidimensional analysis of sales data with respect to annual sales per item type for each of the company branch<br>
<br>Each cell holds an aggregate data value
<br>Data cubes provides fast access to precomputed, summarized data, thereby benefiting on-line analytical processing as well data mining
]]></description><link>cos10022-data-science-principles\(8)-data-preparation\data-cube-aggregation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Data cube aggregation.md</guid><pubDate>Mon, 06 May 2024 02:38:45 GMT</pubDate><enclosure url="lib\media\data-cube-aggregation-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\data-cube-aggregation-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dimensional Reduction]]></title><description><![CDATA[ 
 <br><br>In dimensionality reduction, data encoding or transformations are applied to obtain a reduced or "compressed" representation of the original data<br>Lossless = if the original data can be reconstructed from the compressed data without any loss of information<br>
Lossy = if only an approximation of the original data can be reconstructed from the compressed data<br>Example = Principal Component Analysis (PCA)<br><br>PCA reduces the dimensionality the number of features of a dataset by maintaining as much variance as possible<br><img alt="Dimensional Reduction Image (1).png" src="\lib\media\dimensional-reduction-image-(1).png"><br>
Example = (Gene Expression)<br>
The original expression by 3 genes is projected to two new dimensions, which allow us to draw qualitative conclusions about the separability of experimental conditions]]></description><link>cos10022-data-science-principles\(8)-data-preparation\dimensionality-reduction.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Dimensionality reduction.md</guid><pubDate>Mon, 06 May 2024 03:16:52 GMT</pubDate><enclosure url="lib\media\dimensional-reduction-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\dimensional-reduction-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Numerosity Reduction]]></title><description><![CDATA[ 
 <br><br>Numerosity reduction techniques replace the original data volume by choosing alternative, smaller forms of data representation<br>
e.g. Our grades at university divided into pass, credit, distinction, HD<br>(It can be something like binning)<br>Parametric methods<br>
<br>These methods assume that the data fits some models
<br>Models such as regression and log-linear model are used to estimate the data<br>
<img alt="Parametric method of Numerosity reduction (1).png" src="\lib\media\parametric-method-of-numerosity-reduction-(1).png">
<br>Non-parametric methods<br>
<br>Theses methods do not assume models
<br>Methods such as histogram, clustering, sampling and data cube aggregation are used to store reduces representations of data
<br><br><img alt="Binning and histogram example (1).png" src="\lib\media\binning-and-histogram-example-(1).png"><br>
Binning may have not same width but histogram will always have the same width<br><br><img alt="Clustering Example (1).png" src="\lib\media\clustering-example-(1).png">Clustering use the cluster center to represent the data<br><br><img alt="Sampling, SRSWOR and SRSWR (1).png" src="\lib\media\sampling,-srswor-and-srswr-(1).png"><br>
Sampling can have four methods<br>
<br>SRSWOR (Sampling random sample without replacement)<br>
Cannot have the same data again cause we will not put the data back
<br>SRSWR (Sampling random sample with replacement)<br>
Same data can be drawn again cause after we record, we put the data back to the raw data
<br><img alt="Cluster sample and Stratified sample (1).png" src="\lib\media\cluster-sample-and-stratified-sample-(1).png"><br>
<br>Cluster Sample<br>
We pre-classify the data into different clusters and decide how many samples do you want to have from each of the group equally
<br>Stratified Sample<br>
Tuples in D is divided into mutually disjoint parts called strata (50% of the group in the raw data), with sampling random sampling at each stratum
]]></description><link>cos10022-data-science-principles\(8)-data-preparation\numerosity-reduction.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(8) Data Preparation/Numerosity reduction.md</guid><pubDate>Mon, 06 May 2024 03:42:32 GMT</pubDate><enclosure url="lib\media\parametric-method-of-numerosity-reduction-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\parametric-method-of-numerosity-reduction-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(1) Data Science Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(1) Data Science Overview" data-href="#(1) Data Science Overview" href="\#(1)_Data_Science_Overview" class="internal-link" target="_self" rel="noopener">(1) Data Science Overview</a>

<br><a data-tooltip-position="top" aria-label="Data Structure" data-href="#Data Structure" href="\#Data_Structure" class="internal-link" target="_self" rel="noopener">Data Structure</a>
<br><a data-tooltip-position="top" aria-label="Data Science vs Enterprise Data Warehouse" data-href="#Data Science vs Enterprise Data Warehouse" href="\#Data_Science_vs_Enterprise_Data_Warehouse" class="internal-link" target="_self" rel="noopener">Data Science vs Enterprise Data Warehouse</a>
<br><a data-tooltip-position="top" aria-label="Data Science vs Business Intelligence" data-href="#Data Science vs Business Intelligence" href="\#Data_Science_vs_Business_Intelligence" class="internal-link" target="_self" rel="noopener">Data Science vs Business Intelligence</a>
<br><a data-tooltip-position="top" aria-label="Big Data" data-href="#Big Data" href="\#Big_Data" class="internal-link" target="_self" rel="noopener">Big Data</a>
<br><a data-tooltip-position="top" aria-label="Analytic Sandbox (Workspaces)" data-href="#Analytic Sandbox (Workspaces)" href="\#Analytic_Sandbox_(Workspaces)" class="internal-link" target="_self" rel="noopener">Analytic Sandbox (Workspaces)</a>


<br><br>Data scientist is capable of analyzing and interpreting complex digital data to assist a business in its decision-making<br>Data science is a combination of computer science, math and statistics and domain knowledge<br>
Computer Science + Domain Knowledge = Traditional Software<br>
Computer Science + Math and Statistics = Machine Learning<br>
Math and Statistics + Domain Knowledge = Traditional Research<br><br>Structured = Relational DBMS<br>
Semi-Structured = Extensible Markup Language (XML), JSON, CSV, etc<br>
Quasi-Structured = Web click stream<br>
Unstructured = Text documents, PDFs, images, videos, etc<br><br>Data warehouse is a relational database that is designed for query and analysis rather than for transaction processing. Contains selective, cleaned and transformed historical data<br>Data warehouse include :<br>
<br>ETL (Extraction, Transformation and Loading)
<br>Limitations of the enterprise data warehouse analytics :<br>
<br>High-value data is hard to reach and leverage
<br>Data usually moves in batches from data warehouse to local analytics tools (e.g. R, SAS, Excel)
<br>Data science projects remain isolated and ad-hoc, rather than centrally managed
<br><br>Business intelligence is a more compact thing as we have lower data compared to today.<br>
Now we have plenty of data science to play with that is why data science started to grow. In data science we will encounter things that is more out of the box (More exploratory)<br><br>4 important element of big data :<br>
<br>Timeliness
<br>Scale
<br>Diversity
<br>Accuracy
<br>Big data 4 Vs :<br>
<br>Volume

<br>Scale


<br>Variety

<br>Diversity
<br>Distribution


<br>Velocity

<br>Timeliness


<br>Veracity

<br>Pertaining to the accuracy of the data


<br><br>Sandbox is a concept that we do not want to do experimental things in real world system as if something crashes then everything is destroyed<br>
<br>The sandbox is a simulation environment that can simulate the same thing from the outside world
<br>Data assets can be gathered from multiple sources for analysis
<br>"Analyst owned" rather than "DBA owned"
]]></description><link>cos10022-data-science-principles\(1)-data-science.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(1) Data Science.md</guid><pubDate>Tue, 21 May 2024 12:17:46 GMT</pubDate></item><item><title><![CDATA[(2) Regressions Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(2) Regressions Overview" data-href="#(2) Regressions Overview" href="\#(2)_Regressions_Overview" class="internal-link" target="_self" rel="noopener">(2) Regressions Overview</a>

<br><a data-tooltip-position="top" aria-label="Predictive Models" data-href="#Predictive Models" href="\#Predictive_Models" class="internal-link" target="_self" rel="noopener">Predictive Models</a>
<br><a data-tooltip-position="top" aria-label="Linear regression" data-href="#Linear regression" href="\#Linear_regression" class="internal-link" target="_self" rel="noopener">Linear regression</a>

<br><a data-tooltip-position="top" aria-label="Formula" data-href="#Formula" href="\#Formula" class="internal-link" target="_self" rel="noopener">Formula</a>
<br><a data-tooltip-position="top" aria-label="Anscombe's quartet" data-href="#Anscombe's quartet" href="\#Anscombe's_quartet" class="internal-link" target="_self" rel="noopener">Anscombe's quartet</a>
<br><a data-tooltip-position="top" aria-label="Preparing data for linear regression" data-href="#Preparing data for linear regression" href="\#Preparing_data_for_linear_regression" class="internal-link" target="_self" rel="noopener">Preparing data for linear regression</a>


<br><a data-tooltip-position="top" aria-label="Logistic Regression" data-href="#Logistic Regression" href="\#Logistic_Regression" class="internal-link" target="_self" rel="noopener">Logistic Regression</a>

<br><a data-tooltip-position="top" aria-label="Formula" data-href="#Formula" href="\#Formula" class="internal-link" target="_self" rel="noopener">Formula</a>




<br><br>Regression is the most simple data science model (Linear or Logistic)<br>
Simple and does not require long training time<br>Regression is part of the Model building stage in the <a data-href="(7) Data Analytics Lifecycle" href="\cos10022-data-science-principles\(7)-data-analytics-lifecycle\(7)-data-analytics-lifecycle.html" class="internal-link" target="_self" rel="noopener">(7) Data Analytics Lifecycle</a><br><br>Predictive models are data analytics algorithms used for predicting certain attributes of a given object<br>
The goal of predictive model differs from unsupervised models which are limited to finding specific patterns or structures within the data
<br>Predictive models works by training the data then testing the model with a test dataset<br>
(Training and test datasets are independent of each other)<br><br>Linear regression is one of the oldest supervised / predictive models. Linear regression belongs to what is called parametric learning or parameter modeling<br>
Its goal is to understand the relationship between input and output variables<br>disadvantages :<br>
<br>Can only predict numerical values
<br>Will not work for modeling non-linear relationships
<br><br> a = is moving steepness of the line<br>
b = is where the line starts<br>Linear regression will create a line that go through the data points and if the data points is closer to the line, the result will be more accurate<br>
Linear regression assumes that a linear relationship exists between the input and output variable
<br>Best-fitting regression line is decided from finding the the regression line that minimizes the prediction error. The common measure of such error is sum of the squared errors (SSE)<br>Standard Deviation (S or S)<br>
Standard deviation is telling us how diverse the data is<br>Pearson's correlation coefficient / Collinearity (r)<br>
It measures the strength of association between two variables<br>
1 / -1 = High collinearity (High accuracy)<br>
0 = Low collinearity (Low accuracy)<br><br>Anscombe's quartet is a set of four datasets that have nearly identical simple descriptive statistics (mean, variance) but very different distributions and appearances when graphed<br>Outliers can happen that is why graphing our data is important<br><br>Usually everything in the real world is in Gaussian (normal) distributions (Bell shaped in the data)<br>Gaussian distribution is where the mean is usually 0 there is positive part and negative part<br>
Normal distribution is where the mean value is not 0<br>Some data can be uniform distribution not gaussian distribution<br>Normalisation<br>
It is used to change the observations so that they can be described as normal distribution<br>
Normalisation can use maximum and minimum value to make something to be in the center<br>
<br>Standardisation<br>
It is also called z-score normalisation, which transfors the data so that the resulting distribution has a mean of 0 and a standard deviation of 1<br>
<br><br>Logistic regression is a special case of regression analysis and is calculated when the dependent variable is nominally or ordinally scaled<br><br>Logistic regression uses sigmoid function<br>Sigmoid Function (S)<br>
It only produces 0 (Belongs to one particular group) and 1 (Belongs to the other particular group)<br>
e.g. yes (1) or no (0)
<br>A logistic function or logistic curve is a common S-shaped curve (Sigmoid curve) with equation<br><br>where z stands for the linear regression function]]></description><link>cos10022-data-science-principles\(2)-regressions.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(2) Regressions.md</guid><pubDate>Wed, 22 May 2024 10:10:57 GMT</pubDate></item><item><title><![CDATA[(3) Clustering Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(3) Clustering Overview" data-href="#(3) Clustering Overview" href="\#(3)_Clustering_Overview" class="internal-link" target="_self" rel="noopener">(3) Clustering Overview</a>

<br><a data-tooltip-position="top" aria-label="k-Means Clustering" data-href="#k-Means Clustering" href="\#k-Means_Clustering" class="internal-link" target="_self" rel="noopener">k-Means Clustering</a>

<br><a data-tooltip-position="top" aria-label="The Algorithm" data-href="#The Algorithm" href="\#The_Algorithm" class="internal-link" target="_self" rel="noopener">The Algorithm</a>
<br><a data-tooltip-position="top" aria-label="WSS (Within-Cluster Sum of Squares)" data-href="#WSS (Within-Cluster Sum of Squares)" href="\#WSS_(Within-Cluster_Sum_of_Squares)" class="internal-link" target="_self" rel="noopener">WSS (Within-Cluster Sum of Squares)</a>
<br><a data-tooltip-position="top" aria-label="Silhouette Analysis" data-href="#Silhouette Analysis" href="\#Silhouette_Analysis" class="internal-link" target="_self" rel="noopener">Silhouette Analysis</a>


<br><a data-tooltip-position="top" aria-label="DBSCAN (Data Distribution-oriented Consideration)" data-href="#DBSCAN (Data Distribution-oriented Consideration)" href="\#DBSCAN_(Data_Distribution-oriented_Consideration)" class="internal-link" target="_self" rel="noopener">DBSCAN (Data Distribution-oriented Consideration)</a>
<br><a data-tooltip-position="top" aria-label="The Algorithm" data-href="#The Algorithm" href="\#The_Algorithm" class="internal-link" target="_self" rel="noopener">The Algorithm</a>


<br><br><br>It is an unsupervised learning algorithm used to cluster datasets<br>
The goal is to group the data points in such a way that points in the same cluster are more similar to each other than to those in other clusters.<br>It is not a very stable method as every time we have different initial criteria, we will have different result<br><br>
<br>k-Means k is defined by the user<br>
Choose the value of k for the centroids (Center of mass)
<br>Assignment<br>
For each data point, compute the distance to each centroid and assign the point to the nearest centroid
<br>Update<br>
Calculate the new centroids by computing the mean of the data points in each cluster.
<br>Repeat:<br>
Reassign data points to the nearest centroid and update centroids until convergence.
<br><br>It is the measure of the compactness of the clusters and is used to evaluate the quality of the clustering<br>WSS is often used in conjunction with the elbow method to determine the optimal number of clusters 𝑘<br>Elbow method<br>
Elbow point is where the rate of decrease in WSS slows down significantly. The elbow point represents the optimal number of clusters, beyond which adding more clusters does not result in significant improvement in WSS<br><br>Silhouette Analysis is telling us how diverse our data is<br>Silhouette analysis is a method used to evaluate the quality of clusters created. It provides a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). <br>The silhouette score ranges from -1 (Assigned to the wrong cluster) to 1 (Well-matched to its own cluster)<br><br>It is a popular clustering algorithm used in machine learning and data mining<br>It gives us a better cut for the data even though they are tangling together because it is based on minimum number of points(minPts),  Epsilon(e) <br>
<br>Epsilon (ε): A radius parameter that defines the neighborhood around a data point. Two points are considered neighbors if the distance between them is less than or equal to ε.
<br>MinPts: The minimum number of points required to form a dense region (core point).
<br>Core Point: A point is a core point if it has at least MinPts points (including itself) within its ε-neighborhood.
<br>Border Point: A point that is not a core point but falls within the ε-neighborhood of a core point.
<br>Noise Point: A point that is neither a core point nor a border point, meaning it does not satisfy the density requirement.
<br><br>
<br>Identify Core Points: For each point in the dataset, determine if it is a core point by checking if it has at least MinPts points within its ε-neighborhood.
<br>Cluster Formation:

<br>Start with an arbitrary unvisited point and determine if it is a core point.
<br>If it is a core point, create a new cluster and recursively add all its density-connected neighbors (core points and border points) to this cluster.
<br>If it is not a core point, label it as noise (temporarily; it might be classified later as a border point of another cluster).


<br>Expand Clusters: Continue expanding clusters by visiting each point in the ε-neighborhood of core points until all points have been visited.
]]></description><link>cos10022-data-science-principles\(3)-clustering.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(3) Clustering.md</guid><pubDate>Tue, 21 May 2024 16:50:30 GMT</pubDate></item><item><title><![CDATA[(4) Model Evaluation Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(4) Model Evaluation Overview" data-href="#(4) Model Evaluation Overview" href="\#(4)_Model_Evaluation_Overview" class="internal-link" target="_self" rel="noopener">(4) Model Evaluation Overview</a>

<br><a data-tooltip-position="top" aria-label="Metrics for evaluating supervised models" data-href="#Metrics for evaluating supervised models" href="\#Metrics_for_evaluating_supervised_models" class="internal-link" target="_self" rel="noopener">Metrics for evaluating supervised models</a>

<br><a data-tooltip-position="top" aria-label="Cross Validation" data-href="#Cross Validation" href="\#Cross_Validation" class="internal-link" target="_self" rel="noopener">Cross Validation</a>


<br><a data-tooltip-position="top" aria-label="Metrics for evaluating unsupervised models" data-href="#Metrics for evaluating unsupervised models" href="\#Metrics_for_evaluating_unsupervised_models" class="internal-link" target="_self" rel="noopener">Metrics for evaluating unsupervised models</a>


<br><br>Model Evaluation / Model Validation is an important activity in phase 4, it allows us to determine if the chosen analytic model has performed well in meeting our analytical needs<br><br>Confusion Matrix<br><br>
<br>True Positives (TP): Number of correctly predicted positive cases.
<br>True Negatives (TN): Number of correctly predicted negative cases.
<br>False Positives (FP): Number of incorrectly predicted positive cases.
<br>False Negatives (FN): Number of incorrectly predicted negative cases.
<br>
<br>Accuracy<br>
Accuracy measures the overall correctness of a classification model<br>

<br>True Positive Rate (TPR) - Sensitivity or Recall<br>
Measures the proportion of actual positives that are correctly identified by the model<br>

<br>True Negative Rate (TNR) - Specificity or Selectivity<br>
Measures the proportion of actual negatives that are correctly identified by the model<br>

<br>False Positive Rate (FPR) - Fall-out (Type I Error)<br>
Measures the proportion of actual negatives that are incorrectly classified as positives<br>

<br>Fales Negative Rate (FNR) - Missing Rate (Type II Error)<br>
Measures the proportion of actual positives that are incorrectly classified as negatives<br>

<br>Precision = True Positive / Predictive Condition Positive<br>
Measures the proportion of positive predictions that are actually correct<br>

<br>Area Under the Curve (AUC) - Plasma Concentration-time Profile
<br><br>Three different types of validation<br>
<br>Holdout cross validation ("Percentage split")<br>
Separates a dataset intro training and training sets according to pre-specified percentages
<br>K-fold cross validation<br>
The entire dataset is randomly divided into 𝐾 equally sized (or nearly equally sized) subsets, called folds then for each 𝐾 iteration, train and validate it then in the end evaluate the average performance
<br>Leave-one-out cross validation<br>
This method takes 1 record as the test set and uses all remaining records as the training set
<br><br>Evaluating unsupervised learning models can be more challenging than evaluating supervised models because there are no ground truth labels to directly compare the predictions against<br>We can try to ask domain experts to subjectively inspect the results of an unsupervised model and provide their feedback<br>
There are objective evaluation metrics that can be used however, it is usually differ from one unsupervised model to another<br>e.g.<br>
k-Means clustering uses Silhouette and WSS<br>
Association rules uses Support, Confidence, Lift, Leverage]]></description><link>cos10022-data-science-principles\(4)-model-evaluation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(4) Model Evaluation.md</guid><pubDate>Wed, 22 May 2024 13:16:32 GMT</pubDate></item><item><title><![CDATA[(4) Naïve Bayes Classifier Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(4) Naïve Bayes Classifier Overview" data-href="#(4) Naïve Bayes Classifier Overview" href="\#(4)_Naïve_Bayes_Classifier_Overview" class="internal-link" target="_self" rel="noopener">(4) Naïve Bayes Classifier Overview</a>

<br><a data-tooltip-position="top" aria-label="Bayes' Theorem" data-href="#Bayes' Theorem" href="\#Bayes'_Theorem" class="internal-link" target="_self" rel="noopener">Bayes' Theorem</a>

<br><a data-tooltip-position="top" aria-label="Formula" data-href="#Formula" href="\#Formula" class="internal-link" target="_self" rel="noopener">Formula</a>
<br><a data-tooltip-position="top" aria-label="Use Cases" data-href="#Use Cases" href="\#Use_Cases" class="internal-link" target="_self" rel="noopener">Use Cases</a>
<br><a data-tooltip-position="top" aria-label="Advantages and Disadvantages" data-href="#Advantages and Disadvantages" href="\#Advantages_and_Disadvantages" class="internal-link" target="_self" rel="noopener">Advantages and Disadvantages</a>




<br><br>A Naive Bayes classifier is a probabilistic classifier based on Bayes' theorem<br>
Naive Bayes is in the machine learning under the supervised learning and under the classification class<br><br>Bayes' theorem gives the relationship between the probabilities of two events and their conditional probabilities<br><br>Where :<br>
<br>P(y∣X) is the posterior probability of class 𝑦 given the feature vector 𝑋.
<br>𝑃(𝑋∣𝑦) is the likelihood of feature vector 𝑋 given class 𝑦.
<br>𝑃(𝑦) is the prior probability of class 𝑦.
<br>𝑃(𝑋) is the marginal probability of the feature vector 𝑋.
<br><br>Used in predicting if a or b happen and what does it affect usually used in supermarket buying anaylsis<br>We use frequency table to display the frequency, or count, of occurrences of values within a dataset<br>
It gives us value and based on this value we can find the result<br><br>Advantages:<br>
<br>Simple and Fast: Easy to implement and computationally efficient.
<br>Handles High-Dimensional Data: Performs well with large feature sets, especially in text classification.
<br>Works Well with Small Data: Requires relatively small amounts of training data to estimate the parameters.
<br>Disadvantages:<br>
<br>Independence Assumption: The strong assumption of feature independence is often unrealistic in practice.
<br>May result in loss of useful information
]]></description><link>cos10022-data-science-principles\(4)-naïve-bayes-classifier.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(4) Naïve Bayes Classifier.md</guid><pubDate>Wed, 22 May 2024 08:04:26 GMT</pubDate></item><item><title><![CDATA[(5) Ensemble Learning Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(5) Ensemble Learning Overview" data-href="#(5) Ensemble Learning Overview" href="\#(5)_Ensemble_Learning_Overview" class="internal-link" target="_self" rel="noopener">(5) Ensemble Learning Overview</a>

<br><a data-tooltip-position="top" aria-label="Decision Tree" data-href="#Decision Tree" href="\#Decision_Tree" class="internal-link" target="_self" rel="noopener">Decision Tree</a>

<br><a data-tooltip-position="top" aria-label="Entropy (H)" data-href="#Entropy (H)" href="\#Entropy_(H)" class="internal-link" target="_self" rel="noopener">Entropy (H)</a>
<br><a data-tooltip-position="top" aria-label="Information gain" data-href="#Information gain" href="\#Information_gain" class="internal-link" target="_self" rel="noopener">Information gain</a>
<br><a data-tooltip-position="top" aria-label="CART Algorithm" data-href="#CART Algorithm" href="\#CART_Algorithm" class="internal-link" target="_self" rel="noopener">CART Algorithm</a>
<br><a data-tooltip-position="top" aria-label="Tree Construction Summary" data-href="#Tree Construction Summary" href="\#Tree_Construction_Summary" class="internal-link" target="_self" rel="noopener">Tree Construction Summary</a>
<br><a data-tooltip-position="top" aria-label="Pruning" data-href="#Pruning" href="\#Pruning" class="internal-link" target="_self" rel="noopener">Pruning</a>


<br><a data-tooltip-position="top" aria-label="Ensemble Learning" data-href="#Ensemble Learning" href="\#Ensemble_Learning" class="internal-link" target="_self" rel="noopener">Ensemble Learning</a>

<br><a data-tooltip-position="top" aria-label="Bootstrapping" data-href="#Bootstrapping" href="\#Bootstrapping" class="internal-link" target="_self" rel="noopener">Bootstrapping</a>
<br><a data-tooltip-position="top" aria-label="Boosting" data-href="#Boosting" href="\#Boosting" class="internal-link" target="_self" rel="noopener">Boosting</a>
<br><a data-tooltip-position="top" aria-label="Stacking" data-href="#Stacking" href="\#Stacking" class="internal-link" target="_self" rel="noopener">Stacking</a>


<br><a data-tooltip-position="top" aria-label="Random Forest" data-href="#Random Forest" href="\#Random_Forest" class="internal-link" target="_self" rel="noopener">Random Forest</a>


<br><br>Ensemble learning is a machine learning technique where multiple models are trained to solve the same problem and their predictions are combined to obtain a final prediction<br>To understand how ensemble learning works we need to first understand how decision tree works<br><br>Decision Tree is a supervised machine learning algorithm used for classification and regression tasks which works by recursively partitioning data into subsets based on the values of input features, with the goal of predicting the target variable<br>Decision tree (Treelike structure) consist of :<br>
<br>Root Node
<br>Decision Node (Trunk)
<br>Leaf Nodes
<br><br><br>It is a field of study concerned with quantifying information for communication<br>
Entropy can be greater than 1 if you have two groups
<br><br>Gain = H - (wH + wH)<br>H, H, H = Entropy (In the group)<br>
w, w = Total data set in the group <br><br>It is a measurement for measuring the purity or the actually impurity which is used by CART algorithm called the Gini Index<br>
The impurity equal to 0 does not always mean a good thing in all cases, if a cluster only contains a single data there is no meaning for having the cluster
<br><br>
<br>Which question to ask?
<br>How much a question helps to unmix the labels? (Determine potential and best split)
<br>Quantify the amount of uncertainty at a single node using the Gini impurity metric
<br>Quantify how much a question reduces the uncertainty using Information Gain
<br>Repeating to ask questions until the data can not be divided further
<br>Right size of the tree?<br>
Like many other machine learning algorithms, decision trees are subject to potentially overfitting the training data. The solution here is pruning<br><br>The goal is to have a tree that generalize better. The idea is to cut branches that seems as a result from overfitting the training set<br>
We can use pre-pruning or post-pruning<br>Common Pruning methods :<br>
<br>Reduced Error Pruning
<br>Minimum Description Length Pruning
<br><br>Ensemble learning is a machine learning technique where multiple models are trained to solve the same problem. The predictions from the multiple models are then combined to improve overall performance<br>Commonly used methods :<br>
<br>Bagging (Bootstrapping)
<br>Boosting
<br>Stacking
<br><br>It is random sampling with replacement<br>The output of each predictor(classifier) is summarised by the aggregation function (Statistical mode for classification and average for regression) which helps to reduce bot bias and variance<br><br>Boosting aims to improve the predictive performance of a model by sequentially training multiple weak learners (typically decision trees) and focusing each subsequent learner on the mistakes made by the previous ones<br>It works by :<br>
<br>Initialization
<br>Sequential Training
<br>Weighted Voting (Combining the predictions)
<br><br>It combines multiple base models (learners) to improve predictive performance<br>It works by :<br>
<br>Base models (Multiple diverse base models are trained on a training data (Can use various base models such as, decision tree, neural networks, etc))
<br>Validation Data
<br>Collecting predictions
<br>Meta-model training
<br>Final Prediction
<br><br>Random forest is a powerful ensemble learning method for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the mode (for classification) or mean prediction (for regression) of the individual trees<br>When to use random forest?<br>
<br>Remote sensing by using Enhanced Theromatic Mapping devices to acquire images on the earth's surface
<br>Object detection by multiclass object detection which is done using random forest algorithms
<br>Gaming console, Random forest is used in the KINECT game console (Tracking body movements and recreates it in the game)
<br>Why Random forest?<br>
<br>No overfitting (Use of multiple trees to reduce the risk of overfitting)
<br>High accuracy 
<br>Estimates missing data
<br>How it works?<br>
<br>Bootstrap sampling
<br>Feature randomization
<br>Decision Tree Training
<br>Aggregation for predictions (Majority vote)<br>
<img alt="How Random Forest Work (1).png" src="\lib\media\how-random-forest-work-(1).png">
]]></description><link>cos10022-data-science-principles\(5)-ensemble-learning.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(5) Ensemble Learning.md</guid><pubDate>Wed, 22 May 2024 08:50:03 GMT</pubDate><enclosure url="lib\media\how-random-forest-work-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\how-random-forest-work-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Ethics]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="Data Ethics" data-href="#Data Ethics" href="\#Data_Ethics" class="internal-link" target="_self" rel="noopener">Data Ethics</a>
<br><a data-tooltip-position="top" aria-label="Data Security" data-href="#Data Security" href="\#Data_Security" class="internal-link" target="_self" rel="noopener">Data Security</a>
<br><br>It refers to the moral principles and values that govern the collection, use and dissemination of data<br>Data ethics involves a range of considerations including :<br>
<br>Privacy<br>
Protecting the personal information of individuals and preventing its misuse
<br>Consent<br>
We need to obtain informed consent from  individuals
<br>Transparency<br>
Being transparent how data is collected, used and shared
<br>Bias<br>
Don't be bias in data collection and analysis
<br>Security
<br>Accountability
<br>
Ethics approval is a must before the project can go into the execution phase
<br>Some common steps :<br>
<br>Familiarize yourself with ethical principles<br>
Understand ethical principles that govern research, respect for autonomy, beneficence, non-maleficence and justice
<br>Develop a research proposal<br>
Provide a detailed description of the research question, methods and potential risks and benefits to participants. Must also include plan to obtain informed consent, protect participant privacy and minimize potential harms
<br>Submit application<br>
Submit ethics application to the appropriate research ethics committee or institutional review board (IRB)
<br>Wait for approval
<br>Obtain informed consent
<br>Conduct our research
<br><br>It refers to the protection of digital data such as files, databases, other sensitive information from unauthorized access, use, disclosure, modification or destruction. It involves implementing measures to prevent data breaches, theft and other forms of cyber-attacks that can compromise the confidentiality, integrity and availability of data<br>Data security measures :<br>
<br>Access controls<br>
Limiting access to sensitive data by requiring authentication
<br>Encryption<br>
Encode sensitive data, making it unreadable to unauthorized users
<br>Network security<br>
Securing networks with firewalls, intrusion detection and prevention systems
<br>Physical security<br>
Protecting servers, hard drives and mobile devices with physical access controls
<br>Data backup and recovery<br>
Regularly backing up data to ensure that it can be restored in an event of a data breach or system failure
<br>Data retention policies<br>
Establishing policies and procedures for retaining and disposing of data in accordance with legal and regulatory requirements
]]></description><link>cos10022-data-science-principles\(7)-data-ethics-&amp;-security.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(7) Data Ethics &amp; Security.md</guid><pubDate>Wed, 22 May 2024 10:41:15 GMT</pubDate></item><item><title><![CDATA[(7) Data Store Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="(7) Data Store Overview" data-href="#(7) Data Store Overview" href="\#(7)_Data_Store_Overview" class="internal-link" target="_self" rel="noopener">(7) Data Store Overview</a>

<br><a data-tooltip-position="top" aria-label="Data types" data-href="#Data types" href="\#Data_types" class="internal-link" target="_self" rel="noopener">Data types</a>
<br><a data-tooltip-position="top" aria-label="Categories" data-href="#Categories" href="\#Categories" class="internal-link" target="_self" rel="noopener">Categories</a>


<br><br>The basic unit in computer systems, everything is stored in bits<br>
1 bit is capable of representing "0" and "1"<br><br>Commmonly seen data in our daily life :<br>
<br>Text Data<br>
ASCII code uses 7 bits to encode symbols<br>
Unicode uses 8, 16 or 32 bits include much more symbols in different languages
<br>Image Data<br>
It uses a combination of 3 channel of RGB (3 Main Color) to create an image<br>
Each channel has a single dot = It contains 8 bits<br>
1 Pixel = 3 bytes
<br>Audio Data<br>
Audio signal is composed of two parts, DC component and the AC component<br>
The DC part serves a bias for raising the level of volume. It is usually removed before analysing the signal
<br>Streaming / Video Data<br>
It like stacking up an image in a series along with a timeline<br>
It uses Frame as a measure of the timeline
<br>3D Image
<br>Trajectory Data<br>
It uses 3 elements (Latitude, Longtitude and Timestamp)
<br><br>Data :<br>
<br>Category Data
<br>
<br>Text
<br>
<br>Numerical Data
<br>
<br>Audio
<br>Image
<br>Streaming / Video
<br>Trajectory
<br>Etc
<br>Data based on dimension :<br>
<br>1D
<br>
<br>Text
<br>Audio
<br>
<br>2D
<br>
<br>Image
<br>
<br>3D
<br>
<br>Streaming / Video
<br>Trajectory
<br>Data itself :<br>
<br>Continuous
<br>
<br>Audio
<br>Trajectory (In real world)
<br>
<br>Discrete
<br>Check out <a data-href="Data Types" href="\data-organization\data-types\data-types.html" class="internal-link" target="_self" rel="noopener">Data Types</a>]]></description><link>cos10022-data-science-principles\(7)-data-store.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(7) Data Store.md</guid><pubDate>Wed, 22 May 2024 10:41:47 GMT</pubDate></item><item><title><![CDATA[Data analytics methods]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="Data analytics methods" data-href="#Data analytics methods" href="\#Data_analytics_methods" class="internal-link" target="_self" rel="noopener">Data analytics methods</a>

<br><a data-tooltip-position="top" aria-label="Exploratory Data Analysis (EDA)" data-href="#Exploratory Data Analysis (EDA)" href="\#Exploratory_Data_Analysis_(EDA)" class="internal-link" target="_self" rel="noopener">Exploratory Data Analysis (EDA)</a>

<br><a data-tooltip-position="top" aria-label="Descriptive statistics" data-href="#Descriptive statistics" href="\#Descriptive_statistics" class="internal-link" target="_self" rel="noopener">Descriptive statistics</a>
<br><a data-tooltip-position="top" aria-label="Exploration vs Presentation" data-href="#Exploration vs Presentation" href="\#Exploration_vs_Presentation" class="internal-link" target="_self" rel="noopener">Exploration vs Presentation</a>


<br><a data-tooltip-position="top" aria-label="Statistical Methods for Evaluation" data-href="#Statistical Methods for Evaluation" href="\#Statistical_Methods_for_Evaluation" class="internal-link" target="_self" rel="noopener">Statistical Methods for Evaluation</a>

<br><a data-tooltip-position="top" aria-label="Hypothesis Testing" data-href="#Hypothesis Testing" href="\#Hypothesis_Testing" class="internal-link" target="_self" rel="noopener">Hypothesis Testing</a>
<br><a data-tooltip-position="top" aria-label="Power and Sample Size" data-href="#Power and Sample Size" href="\#Power_and_Sample_Size" class="internal-link" target="_self" rel="noopener">Power and Sample Size</a>
<br><a data-tooltip-position="top" aria-label="Analysis of Variance (ANOVA)" data-href="#Analysis of Variance (ANOVA)" href="\#Analysis_of_Variance_(ANOVA)" class="internal-link" target="_self" rel="noopener">Analysis of Variance (ANOVA)</a>




<br><br><br>EDA is an approach to analysing datasets to summarize their main characteristics, often with visual methods<br>We can easily shows the relationship a data with Survey and visualize<br>
Four important types of data visualization :<br>
<br>Tables
<br>Charts
<br>Maps (Connect data to the physical world)
<br>Graphs (Networks) - Shows the interconnections between various types of real-world objects
<br>Visualizing data can help us easily to :<br>
<br>See the trend even though they have the same mean or median
<br>See dirty data (Noise, Outliers)
<br>See time-specific patterns (Time-series Data), seasonality effect
<br>EDA Usually uses descriptive statistics<br><br>It quantitatively describe the main features of data, it represent something inside a group<br>Main data features :<br>
<br>Measures of central tendency - representing the center<br>
e.g. Mean, median, mode<br>
<img alt="Mean, Median, Mode.png" src="\lib\media\mean,-median,-mode.png">
<br>Measures of variability - representing the spread of the data from the center<br>
e.g. Standard Deviation<br>
<img alt="Standard deviation (1).png" src="\lib\media\standard-deviation-(1).png">
<br>Measures of relative standing - representing the relative position of specific requirements in the data<br>
e.g. Quantiles / Quartiles<br>
<img alt="Relative Standing (1).png" src="\lib\media\relative-standing-(1).png">
<br>Visualizing a single variable can use :<br>
<br>Dotchart
<br>Log10 Regression
<br>Examining multiple variables can use :<br>
<br>Linear regression but if it does not work, we can use the loess curve function
<br>Dotplot to visualize multiple variables, with dotplot with color coding (Scatterplot matrix)
<br>Barplot
<br>Box and whisker
<br>When you encounter a big data type of  problem, it is hard to see the distributed data inside the cluster. Here we can use Hexbinplot, which combines the idea of scatterplot and histogram and uses shading to represent the concentration of data in each hexbin<br>
<img alt="Hexbinplot Example (1).png" src="\lib\media\hexbinplot-example-(1).png">
<br><br>Exploring data and presenting data is two different things<br>
<br>Data scientist prefer graphs that are technical in nature
<br>Nontechnical stakeholders prefer simple, clear graphics that focus on the message rather than the data<br>
e.g. Density plot better for data scientists and histograms better to show to stakeholders
<br><br><br><br>It is a common techniques used to assess the difference of the means from two samples of data or the significance of the difference<br>
The basic concept is to form an assertation and test it with data<br><img alt="Hypothesis Testing Image (1).png" src="\lib\media\hypothesis-testing-image-(1).png"><br>
A hypothesis test leads to either rejecting the H in favour of the H or not rejecting he H<br>
H = null hypothesis<br>
H or H = alternative hypothesis<br>
We prefer to use normal distribution data
<br>Two parametric methods to test the difference in means :<br>
<br>Student's T-Test<br>
It assumes two normally distributed populations have equal but unknown variance
<br>Welch's T-Test (Unequal variance t-test)<br>
It assumes two normally distributed populations have unequal variance
<br>Wilcoxon Rank-Sum Test (Usually used when sample size &lt; 30)
<br>A hypothesis test may result in two types of errors<br>
<br>Type I Error<br>
Rejection of the null hypothesis when it is true
<br>Type II Error<br>
Acceptance of the null hypothesis when the null hypothesis is false
<br><br>The power of a test is the probability of correctly rejecting the null hypothesis<br>
The power of a test improves as the sample size increases<br>In the difference of means, the power of a hypothesis test depends on the true difference of the population means (For a fixed significance level, a larger sample size is required to detect a smaller difference in the means)<br>Effect size = the magnitude of the difference between the means<br>
<br>As the sample size becomes larger, it is easier to detect a given effect size
<br><br>Here we also compare using the means<br>
Compared to T-test we test 1 single group to the overall population but in ANOVA we can compare multiple groups simultaneously<br>
<img alt="ANOVA Example (1).png" src="\lib\media\anova-example-(1).png">]]></description><link>cos10022-data-science-principles\(9)-basic-data-analytics-methods.html</link><guid isPermaLink="false">COS10022 Data Science Principles/(9) Basic Data Analytics Methods.md</guid><pubDate>Wed, 22 May 2024 13:08:45 GMT</pubDate><enclosure url="lib\media\mean,-median,-mode.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\mean,-median,-mode.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Artificial Neural Network (ANN) Overview]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="Artificial Neural Network (ANN) Overview" data-href="#Artificial Neural Network (ANN) Overview" href="\#Artificial_Neural_Network_(ANN)_Overview" class="internal-link" target="_self" rel="noopener">Artificial Neural Network (ANN) Overview</a>

<br><a data-tooltip-position="top" aria-label="Percepton (P)" data-href="#Percepton (P)" href="\#Percepton_(P)" class="internal-link" target="_self" rel="noopener">Percepton (P)</a>

<br><a data-tooltip-position="top" aria-label="Parameters" data-href="#Parameters" href="\#Parameters" class="internal-link" target="_self" rel="noopener">Parameters</a>


<br><a data-tooltip-position="top" aria-label="Feed Forward Neural Network (FFNN)" data-href="#Feed Forward Neural Network (FFNN)" href="\#Feed_Forward_Neural_Network_(FFNN)" class="internal-link" target="_self" rel="noopener">Feed Forward Neural Network (FFNN)</a>
<br><a data-tooltip-position="top" aria-label="Radial Basis Function Neural Network (RBF)" data-href="#Radial Basis Function Neural Network (RBF)" href="\#Radial_Basis_Function_Neural_Network_(RBF)" class="internal-link" target="_self" rel="noopener">Radial Basis Function Neural Network (RBF)</a>
<br><a data-tooltip-position="top" aria-label="Kohonen Self Organising Neural Network" data-href="#Kohonen Self Organising Neural Network" href="\#Kohonen_Self_Organising_Neural_Network" class="internal-link" target="_self" rel="noopener">Kohonen Self Organising Neural Network</a>
<br><a data-tooltip-position="top" aria-label="Recurrent Neural Network (RNN)" data-href="#Recurrent Neural Network (RNN)" href="\#Recurrent_Neural_Network_(RNN)" class="internal-link" target="_self" rel="noopener">Recurrent Neural Network (RNN)</a>
<br><a data-tooltip-position="top" aria-label="Convolution Neural Network (CNN)" data-href="#Convolution Neural Network (CNN)" href="\#Convolution_Neural_Network_(CNN)" class="internal-link" target="_self" rel="noopener">Convolution Neural Network (CNN)</a>


<br><br>It is a learning method under the machine learning subject<br>
An Artificial Neural Network (ANN) is a computational model inspired by the way biological neural networks in the human brain process information<br>They consist of interconnected nodes, called neurons, organized into layers<br><br>It consist of input output node and it can contain hidden layer<br>
Used primarily for binary classification tasks<br>It is cutting the solution into different group (Prediction of category)<br>
Based on the cut there is two types :<br>
<br>Linearly Separable<br>
You can directly use mathematical function here
<br>Not Linearly Seprable<br>
You need to convert it into different coordination system or multiple cuts<br>
e.g. Polar Coordination
<br><br>A perceptron consists of a single neuron and includes the following components:<br>
<br>Input Features (𝑥1,𝑥2,...,𝑥𝑛​): The input values (features) that the perceptron receives.
<br>Weights (𝑤1,𝑤2,...,𝑤𝑛​): Each input feature is associated with a weight that determines its importance.
<br>Bias (𝑏): An additional parameter that allows the activation threshold to be adjusted independently of the input values.
<br>Activation Function: A function that processes the weighted sum of the inputs and produces the output.<br>
where:
<br>
<br>𝑦 is the output of the perceptron.
<br>𝑥𝑖​ are the input features.
<br>𝑤𝑖​ are the weights associated with the input features.
<br>𝑏 is the bias term.
<br>𝑓 is the activation function (usually a step function).
<br>Sigmoid Function, Threshold function, Relu function is used to round the output of the node<br><br>Perceptron is also a type of the FFNN<br>In this architecture, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any), and finally to the output nodes. There are no cycles or loops in the network.<br>Feedforward Neural Networks are foundational models in deep learning, providing the basis for more complex architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).<br><br>A Radial Basis Function Neural Network (RBFNN) is a type of artificial neural network that uses radial basis functions as activation functions. It is particularly well-suited for tasks involving pattern recognition, function approximation, and time-series prediction. The RBFNN is structured differently from traditional feedforward neural networks, primarily in how it processes inputs and generates outputs<br>RBF Neural Networks are powerful tools for specific tasks where the local approximation of the input space is beneficial. Their unique structure and training process make them suitable for applications requiring fast and accurate pattern recognition or function approximation<br>RBF uses neuron as the centroid (Center point of the group)<br>
It use the distribution to represent how far this thing is from the centroid<br><br>The Kohonen Self-Organizing Map (SOM), also known as the Kohonen Map. It is an unsupervised learning algorithm used for clustering and visualization of high-dimensional data. Unlike other neural networks, SOMs are particularly known for their ability to preserve the topological properties of the input space, meaning they maintain the spatial relationships between the input data points in their mapping.<br><br>A Recurrent Neural Network (RNN) is a type of artificial neural network designed to recognize patterns in sequences of data, such as time series, speech, text, and video. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, enabling them to maintain a memory of previous inputs. This makes RNNs particularly well-suited for tasks where the context or sequence of the input data is important.<br><br>A Convolutional Neural Network (CNN) is a type of artificial neural network specifically designed to process and analyze grid-like data structures, such as images. CNNs are particularly effective for tasks involving image and video recognition, image classification, and other tasks that involve spatial hierarchies.]]></description><link>cos10022-data-science-principles\(10)-artificial-neural-network-(ann).html</link><guid isPermaLink="false">COS10022 Data Science Principles/(10) Artificial Neural Network (ANN).md</guid><pubDate>Wed, 22 May 2024 14:03:51 GMT</pubDate></item><item><title><![CDATA[COS10022 Data Science Principles]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="COS10022 Data Science Principles" href="\cos10022-data-science-principles\cos10022-data-science-principles.html" class="internal-link" target="_self" rel="noopener">COS10022 Data Science Principles</a>

<br><a data-href="(1) Data Science" href="\cos10022-data-science-principles\(1)-data-science.html" class="internal-link" target="_self" rel="noopener">(1) Data Science</a>
<br><a data-href="(2) Regressions" href="\cos10022-data-science-principles\(2)-regressions.html" class="internal-link" target="_self" rel="noopener">(2) Regressions</a>
<br><a data-href="(3) Clustering" href="\cos10022-data-science-principles\(3)-clustering.html" class="internal-link" target="_self" rel="noopener">(3) Clustering</a>
<br><a data-href="(4) Model Evaluation" href="\cos10022-data-science-principles\(4)-model-evaluation.html" class="internal-link" target="_self" rel="noopener">(4) Model Evaluation</a>
<br><a data-href="(4) Naïve Bayes Classifier" href="\cos10022-data-science-principles\(4)-naïve-bayes-classifier.html" class="internal-link" target="_self" rel="noopener">(4) Naïve Bayes Classifier</a>
<br><a data-href="(5) Ensemble Learning" href="\cos10022-data-science-principles\(5)-ensemble-learning.html" class="internal-link" target="_self" rel="noopener">(5) Ensemble Learning</a>
<br><a data-href="(6) Association Rules" href="\cos10022-data-science-principles\(6)-association-rules\(6)-association-rules.html" class="internal-link" target="_self" rel="noopener">(6) Association Rules</a>

<br><a data-href="(6) Association Rules" href="\cos10022-data-science-principles\(6)-association-rules\(6)-association-rules.html" class="internal-link" target="_self" rel="noopener">(6) Association Rules</a>
<br><a data-href="Creating Frequent Sets Example" href="\cos10022-data-science-principles\(6)-association-rules\creating-frequent-sets-example.html" class="internal-link" target="_self" rel="noopener">Creating Frequent Sets Example</a>


<br><a data-href="(7) Data Analytics Lifecycle" href="\cos10022-data-science-principles\(7)-data-analytics-lifecycle\(7)-data-analytics-lifecycle.html" class="internal-link" target="_self" rel="noopener">(7) Data Analytics Lifecycle</a>

<br><a data-href="(7) Data Analytics Lifecycle" href="\cos10022-data-science-principles\(7)-data-analytics-lifecycle\(7)-data-analytics-lifecycle.html" class="internal-link" target="_self" rel="noopener">(7) Data Analytics Lifecycle</a>
<br><a data-href="Problem Formulation" href="\cos10022-data-science-principles\(7)-data-analytics-lifecycle\problem-formulation.html" class="internal-link" target="_self" rel="noopener">Problem Formulation</a>


<br><a data-href="(7) Data Ethics &amp; Security" href="\cos10022-data-science-principles\(7)-data-ethics-&amp;-security.html" class="internal-link" target="_self" rel="noopener">(7) Data Ethics &amp; Security</a>
<br><a data-href="(7) Data Store" href="\cos10022-data-science-principles\(7)-data-store.html" class="internal-link" target="_self" rel="noopener">(7) Data Store</a>
<br><a data-href="(8) Data Preparation" href="\cos10022-data-science-principles\(8)-data-preparation\(8)-data-preparation.html" class="internal-link" target="_self" rel="noopener">(8) Data Preparation</a>

<br><a data-href="(8) Data Preparation" href="\cos10022-data-science-principles\(8)-data-preparation\(8)-data-preparation.html" class="internal-link" target="_self" rel="noopener">(8) Data Preparation</a>
<br><a data-href="(Chi-Square) test" href="\cos10022-data-science-principles\(8)-data-preparation\(chi-square)-test.html" class="internal-link" target="_self" rel="noopener">(Chi-Square) test</a>
<br><a data-href="Attribute subset selection" href="\cos10022-data-science-principles\(8)-data-preparation\attribute-subset-selection.html" class="internal-link" target="_self" rel="noopener">Attribute subset selection</a>
<br><a data-href="Correlation coefficient" href="\cos10022-data-science-principles\(8)-data-preparation\correlation-coefficient.html" class="internal-link" target="_self" rel="noopener">Correlation coefficient</a>
<br><a data-href="Covariance" href="\cos10022-data-science-principles\(8)-data-preparation\covariance.html" class="internal-link" target="_self" rel="noopener">Covariance</a>
<br><a data-href="Data cube aggregation" href="\cos10022-data-science-principles\(8)-data-preparation\data-cube-aggregation.html" class="internal-link" target="_self" rel="noopener">Data cube aggregation</a>
<br><a data-href="Dimensionality reduction" href="\cos10022-data-science-principles\(8)-data-preparation\dimensionality-reduction.html" class="internal-link" target="_self" rel="noopener">Dimensionality reduction</a>
<br><a data-href="Numerosity reduction" href="\cos10022-data-science-principles\(8)-data-preparation\numerosity-reduction.html" class="internal-link" target="_self" rel="noopener">Numerosity reduction</a>


<br><a data-href="(9) Basic Data Analytics Methods" href="\cos10022-data-science-principles\(9)-basic-data-analytics-methods.html" class="internal-link" target="_self" rel="noopener">(9) Basic Data Analytics Methods</a>
<br><a data-href="(10) Artificial Neural Network (ANN)" href="\cos10022-data-science-principles\(10)-artificial-neural-network-(ann).html" class="internal-link" target="_self" rel="noopener">(10) Artificial Neural Network (ANN)</a>
<br><a data-href="COS10022 Data Science Principles" href="\cos10022-data-science-principles\cos10022-data-science-principles.html" class="internal-link" target="_self" rel="noopener">COS10022 Data Science Principles</a>


<br><br><br>FolderContains 0 folders, 2 notes.(6) Association RulesFolderContains 0 folders, 3 notes.(7) Data Analytics LifecycleFolderContains 0 folders, 8 notes.(8) Data PreparationNote<a class="internal-link" href="\cos10022-data-science-principles\(1)-data-science.html" target="_self"></a>`table-of-contents5/21/2024, 10:17:46 PMNote<a class="internal-link" href="\cos10022-data-science-principles\(10)-artificial-neural-network-(ann).html" target="_self"></a>`table-of-contents5/23/2024, 12:03:51 AMNote<a class="internal-link" href="\cos10022-data-science-principles\(2)-regressions.html" target="_self"></a>`table-of-contents5/22/2024, 8:10:57 PMNote<a class="internal-link" href="\cos10022-data-science-principles\(3)-clustering.html" target="_self"></a>`table-of-contents5/22/2024, 2:50:30 AMNote<a class="internal-link" href="\cos10022-data-science-principles\(4)-model-evaluation.html" target="_self"></a>`table-of-contents5/22/2024, 11:16:32 PMNote<a class="internal-link" href="\cos10022-data-science-principles\(4)-naïve-bayes-classifier.html" target="_self"></a>`table-of-contents5/22/2024, 6:04:26 PM<a class="internal-link" href="\cos10022-data-science-principles\(5)-ensemble-learning.html" target="_self"></a>`table-of-contents5/22/2024, 6:50:03 PMNote<a class="internal-link" href="\cos10022-data-science-principles\(7)-data-ethics-&amp;-security.html" target="_self"></a>`table-of-contents5/22/2024, 8:41:15 PMNote<a class="internal-link" href="\cos10022-data-science-principles\(7)-data-store.html" target="_self"></a>`table-of-contents5/22/2024, 8:41:47 PM<a class="internal-link" href="\cos10022-data-science-principles\(9)-basic-data-analytics-methods.html" target="_self"></a>`table-of-contents5/22/2024, 11:08:45 PM]]></description><link>cos10022-data-science-principles\cos10022-data-science-principles.html</link><guid isPermaLink="false">COS10022 Data Science Principles/COS10022 Data Science Principles.md</guid><pubDate>Wed, 22 May 2024 13:21:35 GMT</pubDate></item></channel></rss>