<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Data Science]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Data Science</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 21 May 2024 07:27:46 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 21 May 2024 07:27:43 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Association Rules]]></title><description><![CDATA[ 
 <br># Association Rules Overview
Copy<br>Association rules are a fundamental concept in data mining and machine learning<br>
Can be used in recommendation systems, used to discover the potential connection between one product to another<br>
<br>It is an unsupervised learning method
<br>A descriptive, not predictive method
<br>Used to discover interesting relationships in a large dataset
<br>The disclosed relationships are represented as rules or frequent itemsets
<br>Commonly used for mining transactions in database
<br>e.g. Market basket<br>
<img alt="Association rules example image (1).png" src="\lib\media\association-rules-example-image-(1).png"><br>
These are some questions that association rules can help answer :<br>
<br>Which products tends to be purchased together?
<br>Of those customers who are similar to this person, what products do they tend to buy?
<br>Of those customers who have purchased this product, what other similar products do they tend to view or purchase?
<br>e.g. A transaction of three stacks of receipts<br>
The rules suggest that when cereal is purchased 90% of the time milk is purchased, when bread is purchased, 40% of the time milk is purchased also; when milk is purchased, 23% of the time cereal is also purchased<br><br>Each rule is in the form of X -&gt; Y , where X and Y are non-overlapping itemsets<br>
e.g. {Milk, Diaper} -&gt; {Beer}<br>Steps of Generating associations rules :<br>
<br>Find frequent itemsets whose occurrences exceed a predefined minimum support threshold
<br>Derive association rules from those frequent itemsets (With the constraints of minimum confidence threshold)
<br>Support<br>
No. of transactions that contain both X and Y out of total no of transactions<br>
e.g.<br>
Total number of transactions: 7<br>
Transactions with both bread and milk: 3 (Transactions 1, 3, 6)<br>
So the support for {bread, milk} is 3/7 = 0.433/7 = 0.43 or approximately 43%<br>Confidence<br>
No, of transactions that contain both X and Y out of total no, of transactions that contain X<br>
e.g.<br>
Total transactions containing bread: 5 (Transactions 1, 2, 3, 5, 6)<br>
Transactions containing both bread and milk: 3 (Transactions 1, 3, 6)<br>
The confidence for {bread}â†’{milk} is 3/5 = 0.63/5 = 0.6 or 60%.<br><br>A collection of items or individuals entities that contain some kind of relationship<br>An itemset containing k items is called a k-itemset = {item 1, item 2, ..., item k}<br>
e.g. a set of retail items purchased together in one transaction, a set of hyperlinks clicked on by user in a single session<br><br>One of the most fundamental algorithms for generating association rules<br>
One major component of Apriori is support<br>
<br>Given an itemset L, the support of L is the percentage of transactions that contain L
<br>If 80% of all transactions contain itemset {bread}, then the support of {bread} is 0,8
<br>if 60% of all transactions contain itemset {bread, butter}, then the support of {bread, butter} is 0,6
<br>Apriori Property (Downward closure property)<br>
If an item is considered frequent, then any subset of the frequent itemset must also be frequent<br>
<a data-href="Creating Frequent Sets Example" href="\cos10022-data-science-principles\association-rules\creating-frequent-sets-example.html" class="internal-link" target="_self" rel="noopener">Creating Frequent Sets Example</a><br>Main steps of iteration are (C as a candidate itemset of size k and L as a frequent itemset of size k):<br>
<br>Find frequent itemset  (Starting from L)
<br>Join Step : C is generated by joining  with itself (cartesian product  x  )
<br>Prune step : any (K-1) size itemset that is not frequent cannot be a subset of a frequent k size itemset, hence should be removed from C
<br>Frequent set L has been achieved
<br><img alt="Apriori Algorithm Pseudo Code Image (1).png" src="\lib\media\apriori-algorithm-pseudo-code-image-(1).png"><br><br><br>The process of creating association rules is two-staged<br>
First, a set of candidate rule based on frequent itemsets is generated<br>Second, The appropriateness of theses candidate rules are evaluated using :<br>
<br>Confidence
<br>Lift
<br>Leverage
<br><br>Confidence is the measures of certainty or trustworthiness associated with each discovered rule<br>
Mathematically, the percent of transactions that contain both X and Y out of all the transactions that contain X<br>
<br>A relationship may be thought of as interesting when the algorithm identifies the relationship with a measure of confidence greater than or equal to the predefined threshold (the minimum confidence)<br>Confidence has several flaw :<br>
<br>Given a rule X -&gt; Y, confidence considers only the antecedent (X) and the co-occurrence of X and Y
<br>Cannot tell if a rule contains true implication of the relationship or if the rule is purely coincidental
<br><br>Lift measures how many times more often X and Y occur together than expected if they are statistically independent of each other<br>
A measure of how X and Y are really related rather than coincidentally happening together
<br>Lift = 1, then X and Y are statistically independent<br>
Lift &gt; 1, indicates that the degree of usefulness of the rules (A larger value of lift suggests a greater strength of the association between X and Y)<br>e.g. <img alt="Lift example in Association Rules (1).png" src="\lib\media\lift-example-in-association-rules-(1).png"><br><br>Measures the difference in the probability of X and Y appearing together in the dataset compared to what would be expected if X and Y were statistically independent of each other<br><br>Leverage = 0, means that X and Y are statistically independent<br>
Leverage &gt; 0, indicates that the degree of relationship between X and Y (Larger value indicates stronger relationship)<br>e.g. <img alt="Leverage example in association rules (1).png" src="\lib\media\leverage-example-in-association-rules-(1).png"><br><br>
<br>Confidence is able to identify trustworthy rules, but it cannot tell whether a rule is a coincidental
<br>Measures such as lift and leverage not only ensure interesting rules are identified but also filter out the coincidental rules
<br>Support, confidence, lift and leverage ensures the discovery of interesting and strong rules from the sample dataset
<br><br><br>The term market basket analysis refers to a specific implementation of association rules<br>
<br>For better merchandising - Products to include / exclude from inventory each month
<br>Placements of products
<br>Cross-selling, e.g. Bundling items with high correlation together
<br>Promotional programs - Multiple product purchase incentives managed through a loyalty card program
<br>Input : The simple point-of-sale transaction data<br>
Output : Most frequent affinities among items<br>
Example : According to the transaction data<br>
"Customer who bought a laptop computer and a virus protection software, also bought extended service plan 70% of the time"<br>How to use the pattern/knowledge in a practical terms?<br>
<br>Put the items next to each other for ease of finding
<br>Promote the items as a package (Do not put one on sale if the other(s) are on sale)
<br>Place items far apart from each other so that the customer must walk the aisles to search for it, and by doing so potentially seeing and buying other items
<br>Recommender systems - Amazon, Netflix<br>
<br>Clickstream analysis from web usage log files
<br>Website visitors to page X click on links A,B, C more than on links D, E, F
<br>In Medicine<br>
<br>Relationships between symptoms and illnesses
<br>Diagnosis and patient characteristics and treatments (Usually to be used in medical DSS)
<br>Genes and their functions (Usually to be use in genomics projects)
<br><br>The frequent and high confidence itemsets are found by pre-specified minimum support and minimum confidence levels<br>Measures like Lift or Leverage then ensure that interesting rules are identified rather than coincidental ones<br>However, some of the remaining rules may be considered subjectively uninteresting because the don't yield unexpected profitable actions<br>
e.g. Rules like {paper} -&gt; {pencil} are not interesting / meaningful<br>
Domain Knowledge is important here!
<br><br>
<br>Although the Apriori algorithm is easy to understand and implement, some of the rules generated are practically useless
<br>Additionally, some of the rules may be generated due to coincidental relationships between the variables
<br>Measures like confidence, lift and leverage should be used along with human insights to address this problem
<br>Algorithm requires a scan of the entire database to obtain the result. Accordingly, as the database grows, it takes more time to compute in each run
<br>Approaches to improve Apriori's efficiency<br>
<br>
Partitioning<br>
Any itemset that is potentially frequent in a transaction database must be frequent in at least one of the partitions of the transaction database

<br>
Sampling<br>
This extracts a subset of the data with a lower support threshold and uses the subset to perform association rule mining

<br>
Transaction Reduction<br>
A transaction that does not contain frequent k-itemsets is useless in subsequent scans and therefore can be ignored

<br>
Hash-based itemset counting<br>
If the corresponding hashing bucket count of a k-itemset is below a certain threshold, the k-itemset cannot be frequent

<br>
Dynamic itemset counting<br>
Only add new candidate itemsets when all of their subsets are estimated to be frequent

]]></description><link>cos10022-data-science-principles\association-rules\association-rules.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Association Rules/Association Rules.md</guid><pubDate>Wed, 15 May 2024 11:15:47 GMT</pubDate><enclosure url="lib\media\association-rules-example-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\association-rules-example-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Creating Frequent Sets Example]]></title><description><![CDATA[ 
 <br><br>Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore (k+1) - itemsets<br>When using top to bottom method, we check occurrences from a single item denoted by L1, two items denoted by L2 and so forth, until no more frequent k-itemsets can be found<br>
The finding of each Lk requires one full scan of the database! (Which going to make searching a while)
<br>e.g. Transactions<br><br>We define the minimum support 3/5 = 60% which we want our support must be above 3<br>L1 (1-itemsets)<br><br>L2 (2-itemsets) (No need to generate candidates involving Coke or Eggs)<br><br>L3 (3-itemsets) (We don't need to go down further as they are already in a minimal support)<br><br>Another example :<br>
<img alt="Apriori Algorithm Example (2).png" src="\lib\media\apriori-algorithm-example-(2).png">]]></description><link>cos10022-data-science-principles\association-rules\creating-frequent-sets-example.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Association Rules/Creating Frequent Sets Example.md</guid><pubDate>Mon, 29 Apr 2024 04:34:21 GMT</pubDate><enclosure url="lib\media\apriori-algorithm-example-(2).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\apriori-algorithm-example-(2).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Analytics Lifecycle]]></title><description><![CDATA[ 
 <br><br>The real world is chaotic and so are its problems and data, that is why we need an order and sense-making in order to be able to measure the measures of success<br><br><br>Data science team learns the business domain and assesses the resources available to support the project in terms of people, technology, time and data<br>Activities that are important to do here are to frame the business problem as an analytics challenge and formulating initial hypotheses (IHs) to test and begin learning the data<br><br>
<br>Learning the business domain
<br>Assessing the resources available to support the project
<br>Framing the problem<br>
Establishing failure criteria, identify what needs to be achieved in business terms
<br>Identify key stakeholders<br>
This activity asks who will benefit from the project if the project succeeds?
<br>Interviewing the analytics sponsor<br>
Who should be on the team based on the problem to solve, defining the boundaries condition
<br>Developing Initial Hypotheses (HIs)
<br>Identifying potential data sources<br>
Here we also decide the sort of data infrastructure needed for this type of probem
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/03/phase-1-innovation-analytics.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/03/phase-1-innovation-analytics.html" target="_blank">Information Playground: Phase 1 Innovation Analytics (typepad.com)</a><br><br><br><br>Phase 2 requires the presence of an analytics sandbox, here they perform data cleaning as not all data will be in the format that we want. The team performs ETLT (Extraction, Transform and Load)to get the data into the sandbox<br><br>ETL (Extraction, Transform, Load) is to merge different data in different databases into one database. It also clean dirty data that have different formats.<br>
<img alt="ETL Example In the data analytics lifecycle (1).png" src="\lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png"><br>ELT (Extraction, Load, Transform) is the same to merge different data in different databases into one database but with different approach<br>
<img alt="ELT Example in the Data analytics Lifecycle (1).png" src="\lib\media\elt-example-in-the-data-analytics-lifecycle-(1).png"><br><br>
<br>Preparing the analytic sandbox<br>
We need to create an independence space for experimental trial to avoid permanent damage to the data. We usually create 5x or 10x greater space comparing to the original dataset for the data to grow
<br>Performing ETLT<br>
Select proper tools to move that data into the sandbox e.g. Hadoop, MapReduce, Twitter API
<br>Learning about the data<br>
Look if something is wrong in the data, typos, errors, inconsistent data formats. Also create dataset inventory (Creating a data checklist, which data can do what)
<br>Data conditioning<br>
What are the data sources? What are the target fields, columns and attributes? How clean is the data? How consistent are the contents and fields?
<br>Survey and Visualize<br>
Does the data represent the population of interest? Does the data distribution stay consistent over all the data?
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/03/phase-2-innovation-analytics-data-preparation-1.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/03/phase-2-innovation-analytics-data-preparation-1.html" target="_blank">Information Playground: Phase 2 Innovation Analytics: Data Preparation (typepad.com)</a><br><br><br><br>The data science team determines the methods, techniques and workflow. The team also explores the data to learn about the relationships between variables to select the most suitable models<br><br>
<br>Data exploration and variable selection<br>
Understand the variable. Stakeholders and domain experts may have pre-existing yet flawed assumptions about the data, Hence, the data science team's role is to objectively question these assumptions and correct and bias. Capture the most essential predictors!
<br>Model Selection<br>
Common tools to be used = R, SQL Analysis Service, Microsoft SAS / Access
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/05/phase-3-innovation-analytics-.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/05/phase-3-innovation-analytics-.html" target="_blank">Information Playground: Phase 3 Innovation Analytics: Model Planning (typepad.com)</a><br><br><br><br>The team develops datasets from testing, training and production purposes. The team builds and executes models based on the work done in the model planning phase<br><br><img alt="Model Building Workflow (1).png" src="\lib\media\model-building-workflow-(1).png"><br><br>
<br>Develop an analytical model, fit in on the training data and evaluate its performance on the Data<br>
Training data : Used to discover a predictive relationship<br>
Test data : Used to assess the strength and utility of a predictive relationship<br>
Test and training data are independent of each other (Non-overlapping)
<br>Several key important questions to ask are :<br>
<br>Does the model output / behaviour  make sense to the domain experts?
<br>Is a different form of the model required to address the business problem?
<br>Commercial tools used in this phase :<br>
<br>SAS Enterprise Miner
<br>IBM SPSS Modeler
<br>Matlab
<br>Chorus 6
<br>Open Source tools used in this phase :<br>
<br>R (R Commands can be executed in database)
<br>Octave (Computational modelling with some functionalities of Matlab)
<br>Weka (Rich Java API)
<br>Python (Rich machine and data visualisation libraries)
<br>MADlib
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/06/finding-boundary-spanners.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/06/finding-boundary-spanners.html" target="_blank">Information Playground: Phase 4 Innovation Analytics: Finding Boundary Spanners (typepad.com)</a><br><br><br><br>The team determines if the results of the project are a success or a failure based on the criteria developed in Phase 1<br><br>
<br>Compare outcomes to judge whether success or failure<br>
Take into account caveats, assumptions and any limitation of the results
<br>Determine whether the project succeeded or failed in its objectives<br>
Failure should not be considered as a true failure. Rather, it should be viewed as a failure of the data to accept or reject a given hypothesis
<br>Communicate model that addresses the analytical challenge in the most appropriate way<br>
Select the three most significant ones that can be shared and reflect on the implications of these findings and measure the business value
<br>Reflect on the project, the encountered obstacles and what can be improved in the future<br>
Make recommendations for future work or improvements to the existing processes
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/06/phase-5-innovation-analytics-global-knowledge-flight-patterns.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/06/phase-5-innovation-analytics-global-knowledge-flight-patterns.html" target="_blank">Information Playground: Phase 5 Innovation Analytics: Global Knowledge Flight Patterns (typepad.com)</a><br><br><br><br>The team delivers final reports, briefings, code and technical documents. Team may run a pilot project to implement the models in a production environment<br><br>
<br>Set up a pilot project to deploy the work in a controlled way before broadening the work to a full enterprise scale<br>
The risk can be better managed this way and in a way adjustments can still be made before a full deployment
<br>Manage the deployment scope<br>
Consider running a model in a production environment for a discrete set of products. Minimising risks, while allowing to fine-tune the model
<br>Create mechanism for performing ongoing monitoring of model accuracy and if accuracy degrades, find ways to retrain the model
<br><br>Here we are going to use the introduction the innovation analytics case study at EMC Corp<br>
<a data-tooltip-position="top" aria-label="https://stevetodd.typepad.com/my_weblog/2012/07/phase-6-innovation-analytics-operationalize.html" rel="noopener" class="external-link" href="https://stevetodd.typepad.com/my_weblog/2012/07/phase-6-innovation-analytics-operationalize.html" target="_blank">Information Playground: Phase 6 Innovation Analytics: Operationalize (typepad.com)</a><br><br>]]></description><link>cos10022-data-science-principles\data-analytics-lifecycle\data-analytics-lifecycle.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Analytics Lifecycle/Data Analytics Lifecycle.md</guid><pubDate>Thu, 02 May 2024 12:12:38 GMT</pubDate><enclosure url="lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\etl-example-in-the-data-analytics-lifecycle-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Analytics Lifecycle Canvas]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br>Do I have enough information to draft an analytic plan?<br>Do I have enough "good" data to start building the model?<br>Do I have a good idea about the type of model to try? Can I refine my analytical plan?<br>Is the model robust enough? Have we failed enough?]]></description><link>cos10022-data-science-principles\data-analytics-lifecycle\data-analytics-lifecycle-canvas.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Analytics Lifecycle/Data Analytics Lifecycle Canvas.canvas</guid><pubDate>Thu, 02 May 2024 02:44:28 GMT</pubDate></item><item><title><![CDATA[Chi-Square test]]></title><description><![CDATA[ 
 <br><br>Chi-squared (X) test for categorical data<br>
<img alt="Chi-squared formula Image (1).png" src="\lib\media\chi-squared-formula-image-(1).png"><br>
<img alt="Chi squared example (1).png" src="\lib\media\chi-squared-example-(1).png">Hypothesis for the example : Gender and preferred reading are independent<br>
<img alt="Chi squared test example Image (2).png" src="\lib\media\chi-squared-test-example-image-(2).png">]]></description><link>cos10022-data-science-principles\data-preparation\(chi-square)-test.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/(Chi-Square) test.md</guid><pubDate>Sat, 04 May 2024 12:07:34 GMT</pubDate><enclosure url="lib\media\chi-squared-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\chi-squared-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Attribute subset selection]]></title><description><![CDATA[ 
 <br><br>Attribute subset selection reduces the data set size by removing irrelevant or redundant attributes<br>Heuristic methods that explore reduced search space to find "good" subset of the original attributes :<br>
<br><a data-href="#Stepwise forward selection" href="\#Stepwise_forward_selection" class="internal-link" target="_self" rel="noopener">Stepwise forward selection</a>
<br><a data-href="#Stepwise backward elimination" href="\#Stepwise_backward_elimination" class="internal-link" target="_self" rel="noopener">Stepwise backward elimination</a>
<br><a data-href="#Combination of forward and backward" href="\#Combination_of_forward_and_backward" class="internal-link" target="_self" rel="noopener">Combination of forward and backward</a>
<br>Decision tree induction
<br>The attributes are typically determined using tests of statistical significance, which assumes that the attributes are independent of one another<br>
Other attribute evaluation such as information gain is used in building decision trees for classification<br><br>
<br>Start with an empty set of attributes
<br>Determine the best of the original attributes and add it to the reduces set
<br>At each step, add the best of the remaining original attributes to the reduced set
<br><br>
<br>Start with the full set of attributes
<br>At each step, remove the worst attribute remaining in the set
<br><br>
<br>Start with an empty set of attributes
<br>At each step, add the best attribute to the reduced set and removes the worst from among the remaining attributes<br>
<img alt="Forward and backward elimination and selection Image (1).png" src="\lib\media\forward-and-backward-elimination-and-selection-image-(1).png">
<br><br>Decision tree induction constructs a flowchart-like structure where each internal (nonleaf) node denotes a test on an attribute, each branch corresponds to an outcome of the test, and each external (leaf) node denotes a class prediction.<br>
Decision tree induction is also almost always a binary tree (Two option tree)<br>
<img alt="Decision tree induction example (1).png" src="\lib\media\decision-tree-induction-example-(1).png"><br>
All attributes that do not appear in the tree are assumed to be irrelevant
]]></description><link>cos10022-data-science-principles\data-preparation\attribute-subset-selection.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Attribute subset selection.md</guid><pubDate>Wed, 15 May 2024 22:28:12 GMT</pubDate><enclosure url="lib\media\forward-and-backward-elimination-and-selection-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\forward-and-backward-elimination-and-selection-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Correlation coefficient]]></title><description><![CDATA[ 
 <br><br>Correlation coefficient (r) for numerical data (Pearson's product moment coefficient)<br>
<img alt="Correlation Coefficient Formula Image (1).png" src="\lib\media\correlation-coefficient-formula-image-(1).png"><br>We can visually evaluate correlation using scatter plots<br>
scatterplots shows the correlation coefficient from -1 to 1<br>
r = 1.0 = A perfect positive relationship<br>
r = 0 = No relationship<br>
r = -1.0 = A perfect negative relationship]]></description><link>cos10022-data-science-principles\data-preparation\correlation-coefficient.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Correlation coefficient.md</guid><pubDate>Sat, 04 May 2024 11:49:10 GMT</pubDate><enclosure url="lib\media\correlation-coefficient-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\correlation-coefficient-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Covariance]]></title><description><![CDATA[ 
 <br><br>Covariance (Cov) for numerical data<br>
<img alt="Covariance Formula Image (1).png" src="\lib\media\covariance-formula-image-(1).png"><br>
We can visually evaluate covariance between two variables using scatter plot<br>
Cov(A, B) &lt; 0 : A and B tend to move in the opposite direction<br>
Cov(A, B) &gt; 0 : A and B tend to move in the same direction<br>
Cov(A, B) = 0 : A and B are independent]]></description><link>cos10022-data-science-principles\data-preparation\covariance.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Covariance.md</guid><pubDate>Sat, 04 May 2024 11:52:52 GMT</pubDate><enclosure url="lib\media\covariance-formula-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\covariance-formula-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Cube Aggregation]]></title><description><![CDATA[ 
 <br><br>Aggregating several 2D table into a 3D table<br>
The resulting dataset is smaller in volume without loss of information necessary for the analysis task<br><img alt="Data Cube Aggregation Image (1).png" src="\lib\media\data-cube-aggregation-image-(1).png"><br>
e.g. An electronic company sales per quarter from year 2002 to 2004<br>
Data cubes store multidimensional analysis of sales data with respect to annual sales per item type for each of the company branch<br>
<br>Each cell holds an aggregate data value
<br>Data cubes provides fast access to precomputed, summarized data, thereby benefiting on-line analytical processing as well data mining
]]></description><link>cos10022-data-science-principles\data-preparation\data-cube-aggregation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Data cube aggregation.md</guid><pubDate>Mon, 06 May 2024 02:38:45 GMT</pubDate><enclosure url="lib\media\data-cube-aggregation-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\data-cube-aggregation-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Preparation Overview]]></title><description><![CDATA[ 
 <br><br>Given the presence of an analytics sandbox, the data science team-work with data and perform analytics for the duration of the project<br>Why is data preparation important?<br>
Well-prepared data set will saves us from debugging and additional works in the project. An organized data will be easily to be tracked and accessed<br>Data preparation must contain data quality :<br>
<br>Accuracy
<br>Completeness
<br>Consistency
<br>Timeliness
<br>Believability
<br>Interpretability
<br>Major tasks in data preparation :<br>
<br><a data-href="#Data Cleaning" href="\#Data_Cleaning" class="internal-link" target="_self" rel="noopener">Data Cleaning</a>
<br><a data-href="#Data Integration" href="\#Data_Integration" class="internal-link" target="_self" rel="noopener">Data Integration</a>
<br><a data-href="#Data Transformation" href="\#Data_Transformation" class="internal-link" target="_self" rel="noopener">Data Transformation</a>
<br><a data-href="#Data Reduction" href="\#Data_Reduction" class="internal-link" target="_self" rel="noopener">Data Reduction</a>
<br><br><br>Real-world data is dirty<br>
Here we fill in missing values, smooth noisy data, identify outliers and resolve inconsistency<br><br>This can occur because a number of reasons :<br>
<br>Attributes of interest may not always be available
<br>Not recorded due to equipment malfunctions
<br>Data that may have been deleted
<br>Missing data, particularly for tuples with missing values for some attributes, may need to be inferred<br>6 Ways of handling incomplete data :<br>
<br>Ignore the tuple
<br>Fill in the missing values with side information
<br>Use a global constant to fill in the missing values (N/A)
<br>Use a measure of central tendency for the attribute (e.g. the mean or medium)
<br>Use the attribute mean or medium for all the samples belonging to the same class as the give tuples
<br>Use the most probable value to fill in the missing value<br>
This may be determined with regression, interference-based tools using bayesian formalism or decision tree induction<br>
e.g. using the other similar attributes, we may construct a decision tree
<br><br>Noisy data is a random error or a variance in a measured variable<br>There are several ways to calibrate the data :<br>
<br>Binning (Data Smoothing)<br>
Smoothing the data by consulting its "Neighborhood", the values around it<br>
The sorted values are distributed into a number of "buckets" or "bins"<br>
More bins = higher resolution
<br>Regression<br>
A technique that conforms data values to a function, finding the "best" line to fit two attributes so that one attribute can be used to predict the others
<br>Sliding Window (Moving Average) (Convolution)<br>
Using the neighbourhood data to find the average, this requires to slide through the whole data in sequence
<br>Outlier Analysis<br>
Outliers may also be detected by clustering, where similar values are organized into groups or clusters
<br>Incorrect attributes value may be due to : <br>
<br>Faulty equipment
<br>Data entry problems
<br>Other data problems that require data cleaning :<br>
<br>Duplicate Records
<br>Incomplete Data
<br>Inconsistent Data
<br><br>The first step in data cleaning as a process is discrepancy detection<br>
Discrepancy can be caused by :<br>
<br>Human errors in data entry
<br>Deliberate errors
<br>Data decay (Outdated data)
<br>System errors
<br>How to detect data discrepancy?<br>
<br>Metadata<br>
What are the acceptable values for each attribute? 
<br>Check uniqueness rule, consecutive rule and null rule<br>
Use special characters to replace missing values
<br>Use commercial tool (e.g. Microsoft BI)<br>
Data Scrubbing (Use domain knowledge)<br>
Data Auditing (Correlation and Clustering to find outliers)
<br>Data migration and integration<br>
ETL tools that can specify transformations through a graphical user interface
<br><br><br>Data integration combines data from multiple sources into a coherent store, as in data warehousing<br>
We need to also solve the data value conflicts (American metrics vs British metrics)<br>How can equivalent real-world entities from multiple data sources be matched up? (Entity Identification Problem)<br>
e.g. Customer Id in one database and customer number in a another database<br>Redundant data often occurs between databases<br>
<br>Object identification (The same attribute may have have different names in different databases)
<br>Derivable Data (One attribute may be a "derived" attribute in another table)
<br>Redundant data can also be detected by correlation analysis<br>
Categorical data = X <a data-href="(Chi-Square) test" href="\cos10022-data-science-principles\data-preparation\(chi-square)-test.html" class="internal-link" target="_self" rel="noopener">(Chi-Square) test</a><br>
Numerical data = <a data-href="Correlation coefficient" href="\cos10022-data-science-principles\data-preparation\correlation-coefficient.html" class="internal-link" target="_self" rel="noopener">Correlation coefficient</a>, Covariance<br><br><br>Data reduction is obtaining a reduced representation of the data set that is much smaller in volume but yet produce the same analytical results<br>A data warehouse may store terabytes of data. As we do complex data analysis / mining resulting in a very long time to run on the complete data set<br>Several data reduction strategies :<br>
<br><a data-href="Data cube aggregation" href="\cos10022-data-science-principles\data-preparation\data-cube-aggregation.html" class="internal-link" target="_self" rel="noopener">Data cube aggregation</a>
<br><a data-href="Attribute subset selection" href="\cos10022-data-science-principles\data-preparation\attribute-subset-selection.html" class="internal-link" target="_self" rel="noopener">Attribute subset selection</a>
<br><a data-href="Dimensionality reduction" href="\cos10022-data-science-principles\data-preparation\dimensionality-reduction.html" class="internal-link" target="_self" rel="noopener">Dimensionality reduction</a>
<br><a data-href="Numerosity reduction" href="\cos10022-data-science-principles\data-preparation\numerosity-reduction.html" class="internal-link" target="_self" rel="noopener">Numerosity reduction</a>
<br>Discretization and concept hierarchy generation
<br><br><br>There are several ways to transform the data :<br>
<br>Smoothing
<br>Attribute / Feature construction
<br>Aggregation
<br>Nomalization
<br>Discretization
<br>Concept Hierarchy generation for nominal data
<br><br>Normalization are useful for classification algorithms :<br>
<br>When using Neural network backpropagation algorithm for classification mining, this helps to speed up the learning phase
<br>When using distance-based method for clustering, this helps prevent attributes with initially large range from outweighing attributes with initially smaller ranges
<br>How to do Normalization?<br>
<img alt="How to do normalization (1).png" src="\lib\media\how-to-do-normalization-(1).png"><br>
<br>Min - Max<br>
Causes us to transform the data into between [0,1]
<br>Z-score<br>
It is useful when the actual min and max of attribute are unknown
<br>Decimal scaling (Least used)<br>
Usually used when we convert different measurements, transforming data from [-1,1]
<br><br>It transforms numeric data by mapping values to interval or concept label<br>Discretization techniques :<br>
<br>Binning
<br>Histogram analysis
<br>Cluster analysis
<br>Decision tree analysis
<br>Correlation analysis
<br>For nominal data :<br>
<br>Concept hierarchy
<br><br>It uses a top-down splitting approach<br>
Supervised = makes use of the class label<br>
It also uses entropy to determine split point<br><img alt="Decision Tree Analysis (1).png" src="\lib\media\decision-tree-analysis-(1).png"><br><br>It uses a bottom-up merge approach<br>
supervised = make use of the class label<br>ChiMerge = find the best neighboring intervals (Those having similar distributions of classes)<br><br>Nominal attributes have a finite(but possibly large) number of distinct values, with no ordering among the values<br>
It reduce the data by collecting and replacing low level concepts such as numeric values for age by higher level concepts (youth, adult, senior)<br>
<img alt="Hierarchy Concept Generation Example (1).png" src="\lib\media\hierarchy-concept-generation-example-(1).png">]]></description><link>cos10022-data-science-principles\data-preparation\data-preparation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Data Preparation.md</guid><pubDate>Mon, 06 May 2024 04:16:30 GMT</pubDate><enclosure url="lib\media\how-to-do-normalization-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\how-to-do-normalization-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dimensional Reduction]]></title><description><![CDATA[ 
 <br><br>In dimensionality reduction, data encoding or transformations are applied to obtain a reduced or "compressed" representation of the original data<br>Lossless = if the original data can be reconstructed from the compressed data without any loss of information<br>
Lossy = if only an approximation of the original data can be reconstructed from the compressed data<br>Example = Principal Component Analysis (PCA)<br><br>PCA reduces the dimensionality the number of features of a dataset by maintaining as much variance as possible<br><img alt="Dimensional Reduction Image (1).png" src="\lib\media\dimensional-reduction-image-(1).png"><br>
Example = (Gene Expression)<br>
The original expression by 3 genes is projected to two new dimensions, which allow us to draw qualitative conclusions about the separability of experimental conditions]]></description><link>cos10022-data-science-principles\data-preparation\dimensionality-reduction.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Dimensionality reduction.md</guid><pubDate>Mon, 06 May 2024 03:16:52 GMT</pubDate><enclosure url="lib\media\dimensional-reduction-image-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\dimensional-reduction-image-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Numerosity Reduction]]></title><description><![CDATA[ 
 <br><br>Numerosity reduction techniques replace the original data volume by choosing alternative, smaller forms of data representation<br>
e.g. Our grades at university divided into pass, credit, distinction, HD<br>(It can be something like binning)<br>Parametric methods<br>
<br>These methods assume that the data fits some models
<br>Models such as regression and log-linear model are used to estimate the data<br>
<img alt="Parametric method of Numerosity reduction (1).png" src="\lib\media\parametric-method-of-numerosity-reduction-(1).png">
<br>Non-parametric methods<br>
<br>Theses methods do not assume models
<br>Methods such as histogram, clustering, sampling and data cube aggregation are used to store reduces representations of data
<br><br><img alt="Binning and histogram example (1).png" src="\lib\media\binning-and-histogram-example-(1).png"><br>
Binning may have not same width but histogram will always have the same width<br><br><img alt="Clustering Example (1).png" src="\lib\media\clustering-example-(1).png">Clustering use the cluster center to represent the data<br><br><img alt="Sampling, SRSWOR and SRSWR (1).png" src="\lib\media\sampling,-srswor-and-srswr-(1).png"><br>
Sampling can have four methods<br>
<br>SRSWOR (Sampling random sample without replacement)<br>
Cannot have the same data again cause we will not put the data back
<br>SRSWR (Sampling random sample with replacement)<br>
Same data can be drawn again cause after we record, we put the data back to the raw data
<br><img alt="Cluster sample and Stratified sample (1).png" src="\lib\media\cluster-sample-and-stratified-sample-(1).png"><br>
<br>Cluster Sample<br>
We pre-classify the data into different clusters and decide how many samples do you want to have from each of the group equally
<br>Stratified Sample<br>
Tuples in D is divided into mutually disjoint parts called strata (50% of the group in the raw data), with sampling random sampling at each stratum
]]></description><link>cos10022-data-science-principles\data-preparation\numerosity-reduction.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Preparation/Numerosity reduction.md</guid><pubDate>Mon, 06 May 2024 03:42:32 GMT</pubDate><enclosure url="lib\media\parametric-method-of-numerosity-reduction-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\parametric-method-of-numerosity-reduction-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data analytics methods]]></title><description><![CDATA[ 
 <br><br><br>EDA is an approach to analysing datasets to summarize their main characteristics, often with visual methods<br>We can easily shows the relationship a data with Survey and visualize<br>
Four important types of data visualization :<br>
<br>Tables
<br>Charts
<br>Maps (Connect data to the physical world)
<br>Graphs (Networks) - Shows the interconnections between various types of real-world objects
<br>Visualizing data can help us easily to :<br>
<br>See the trend even though they have the same mean or median
<br>See dirty data (Noise, Outliers)
<br>See time-specific patterns (Time-series Data), seasonality effect
<br>EDA Usually uses descriptive statistics<br><br>It quantitatively describe the main features of data, it represent something inside a group<br>Main data features :<br>
<br>Measures of central tendency - representing the center<br>
e.g. Mean, median, mode<br>
<img alt="Mean, Median, Mode.png" src="\lib\media\mean,-median,-mode.png">
<br>Measures of variability - representing the spread of the data from the center<br>
e.g. Standard Deviation<br>
<img alt="Standard deviation (1).png" src="\lib\media\standard-deviation-(1).png">
<br>Measures of relative standing - representing the relative position of specific requirements in the data<br>
e.g. Quantiles / Quartiles<br>
<img alt="Relative Standing (1).png" src="\lib\media\relative-standing-(1).png">
<br>Visualizing a single variable can use :<br>
<br>Dotchart
<br>Log10 Regression
<br>Examining multiple variables can use :<br>
<br>Linear regression but if it does not work, we can use the loess curve function
<br>Dotplot to visualize multiple variables, with dotplot with color coding (Scatterplot matrix)
<br>Box and whisker
<br>When you encounter a big data type of  problem, it is hard to see the distributed data inside the cluster. Here we can use Hexbinplot, which combines the idea of scatterplot and histogram and uses shading to represent the concentration of data in each hexbin<br>
<img alt="Hexbinplot Example (1).png" src="\lib\media\hexbinplot-example-(1).png">
<br><br>Exploring data and presenting data is two different things<br>
<br>Data scientist prefer graphs that are technical in nature
<br>Nontechnical stakeholders prefer simple, clear graphics that focus on the message rather than the data<br>
e.g. Density plot better for data scientists and histograms better to show to stakeholders
<br><br><br><br>It is a common techniques used to assess the difference of the means from two samples of data or the significance of the difference<br>
The basic concept is to form an assertation and test it with data<br><img alt="Hypothesis Testing Image (1).png" src="\lib\media\hypothesis-testing-image-(1).png"><br>
A hypothesis test leads to either rejecting the H in favour of the H or not rejecting he H<br>
H = null hypothesis<br>
H or H = alternative hypothesis<br>
We prefer to use normal distribution data
<br>Two parametric methods to test the difference in means :<br>
<br>Student's T-Test<br>
It assumes two normally distributed populations have equal but unknown variance
<br>Welch's T-Test (Unequal variance t-test)<br>
It assumes two normally distributed populations have unequal variance
<br>Wilcoxon Rank-Sum Test (Usually used when sample size &lt; 30)
<br>A hypothesis test may result in two types of errors<br>
<br>Type I Error<br>
Rejection of the null hypothesis when it is true
<br>Type II Error<br>
Acceptance of the null hypothesis when the null hypothesis is false
<br><br>The power of a test is the probability of correctly rejecting the null hypothesis<br>
The power of a test improves as the sample size increases<br>In the difference of means, the power of a hypothesis test depends on the true difference of the population means (For a fixed significance level, a larger sample size is required to detect a smaller difference in the means)<br>Effect size = the magnitude of the difference between the means<br>
<br>As the sample size becomes larger, it is easier to detect a given effect size
<br><br>Here we also compare using the means<br>
Compared to T-test we test 1 single group to the overall population but in ANOVA we can compare multiple groups simultaneously<br>
<img alt="ANOVA Example (1).png" src="\lib\media\anova-example-(1).png">]]></description><link>cos10022-data-science-principles\basic-data-analytics-methods.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Basic Data Analytics Methods.md</guid><pubDate>Mon, 06 May 2024 11:46:40 GMT</pubDate><enclosure url="lib\media\mean,-median,-mode.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\mean,-median,-mode.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[COS10022 Data Science Principles Overview]]></title><description><![CDATA[ 
 <br><br>FolderContains 0 folders, 2 notes.Association RulesFolderContains 0 folders, 2 notes.Data Analytics LifecycleFolderContains 0 folders, 8 notes.Data Preparation<a class="internal-link" href="\cos10022-data-science-principles\basic-data-analytics-methods.html" target="_self"></a>EDA is an approach to analysing datasets to summarize their main...5/6/2024, 9:46:40 PMNote<a class="internal-link" href="\cos10022-data-science-principles\data-ethics.html" target="_self"></a>It refers to the moral principles and values that govern the col...5/2/2024, 11:08:16 PMNote<a class="internal-link" href="\cos10022-data-science-principles\data-security.html" target="_self"></a>It refers to the protection of digital data such as files, datab...5/2/2024, 11:13:38 PMNote<a class="internal-link" href="\cos10022-data-science-principles\data-store.html" target="_self"></a>The basic unit in computer systems, everything is stored in bits5/2/2024, 10:52:17 PM<a class="internal-link" href="\cos10022-data-science-principles\ensemble-learning.html" target="_self"></a>Ensemble learning is a machine learning technique where multiple...5/15/2024, 3:26:08 PMNote<a class="internal-link" href="\cos10022-data-science-principles\problem-formulation.html" target="_self"></a>Using proper techniques to solve the problem5/2/2024, 12:26:49 PM]]></description><link>cos10022-data-science-principles\cos10022-data-science-principles.html</link><guid isPermaLink="false">COS10022 Data Science Principles/COS10022 Data Science Principles.md</guid><pubDate>Thu, 02 May 2024 12:39:27 GMT</pubDate></item><item><title><![CDATA[Data Ethics]]></title><description><![CDATA[ 
 <br><br>It refers to the moral principles and values that govern the collection, use and dissemination of data<br>Data ethics involves a range of considerations including :<br>
<br>Privacy<br>
Protecting the personal information of individuals and preventing its misuse
<br>Consent<br>
We need to obtain informed consent from  individuals
<br>Transparency<br>
Being transparent how data is collected, used and shared
<br>Bias<br>
Don't be bias in data collection and analysis
<br>Security
<br>Accountability
<br>
Ethics approval is a must before the project can go into the execution phase
<br>Some common steps :<br>
<br>Familiarize yourself with ethical principles<br>
Understand ethical principles that govern research, respect for autonomy, beneficence, non-maleficence and justice
<br>Develop a research proposal<br>
Provide a detailed description of the research question, methods and potential risks and benefits to participants. Must also include plan to obtain informed consent, protect participant privacy and minimize potential harms
<br>Submit application<br>
Submit ethics application to the appropriate research ethics committee or institutional review board (IRB)
<br>Wait for approval
<br>Obtain informed consent
<br>Conduct our research
]]></description><link>cos10022-data-science-principles\data-ethics.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Ethics.md</guid><pubDate>Thu, 02 May 2024 13:08:16 GMT</pubDate></item><item><title><![CDATA[Data Security]]></title><description><![CDATA[ 
 <br><br>It refers to the protection of digital data such as files, databases, other sensitive information from unauthorized access, use, disclosure, modification or destruction. It involves implementing measures to prevent data breaches, theft and other forms of cyber-attacks that can compromise the confidentiality, integrity and availability of data<br>Data security measures :<br>
<br>Access controls<br>
Limiting access to sensitive data by requiring authentication
<br>Encryption<br>
Encode sensitive data, making it unreadable to unauthorized users
<br>Network security<br>
Securing networks with firewalls, intrusion detection and prevention systems
<br>Physical security<br>
Protecting servers, hard drives and mobile devices with physical access controls
<br>Data backup and recovery<br>
Regularly backing up data to ensure that it can be restored in an event of a data breach or system failure
<br>Data retention policies<br>
Establishing policies and procedures for retaining and disposing of data in accordance with legal and regulatory requirements
]]></description><link>cos10022-data-science-principles\data-security.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Security.md</guid><pubDate>Thu, 02 May 2024 13:13:38 GMT</pubDate></item><item><title><![CDATA[Data Store]]></title><description><![CDATA[ 
 <br><br>The basic unit in computer systems, everything is stored in bits<br>
1 bit is capable of representing "0" and "1"<br><br>Commmonly seen data in our daily life :<br>
<br>Text Data<br>
ASCII code uses 7 bits to encode symbols<br>
Unicode uses 8, 16 or 32 bits include much more symbols in different languages
<br>Image Data<br>
It uses a combination of 3 channel of RGB (3 Main Color) to create an image<br>
Each channel has a single dot = It contains 8 bits<br>
1 Pixel = 3 bytes
<br>Audio Data<br>
Audio signal is composed of two parts, DC component and the AC component<br>
The DC part serves a bias for raising the level of volume. It is usually removed before analysing the signal
<br>Streaming / Video Data<br>
It like stacking up an image in a series along with a timeline<br>
It uses Frame as a measure of the timeline
<br>3D Image
<br>Trajectory Data<br>
It uses 3 elements (Latitude, Longtitude and Timestamp)
<br><br>Data :<br>
<br>Category Data
<br>
<br>Text
<br>
<br>Numerical Data
<br>
<br>Audio
<br>Image
<br>Streaming / Video
<br>Trajectory
<br>Etc
<br>Data based on dimension :<br>
<br>1D
<br>
<br>Text
<br>Audio
<br>
<br>2D
<br>
<br>Image
<br>
<br>3D
<br>
<br>Streaming / Video
<br>Trajectory
<br>Data itself :<br>
<br>Continuous
<br>
<br>Audio
<br>Trajectory (In real world)
<br>
<br>Discrete
<br>Check out <a data-href="Data Types" href="\data-organization\data-types\data-types.html" class="internal-link" target="_self" rel="noopener">Data Types</a>]]></description><link>cos10022-data-science-principles\data-store.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Data Store.md</guid><pubDate>Thu, 02 May 2024 12:52:17 GMT</pubDate></item><item><title><![CDATA[Ensemble Learning]]></title><description><![CDATA[ 
 <br><br>Ensemble learning is a machine learning technique where multiple models are trained to solve the same problem and their predictions are combined to obtain a final prediction<br>To understand how ensemble learning works we need to first understand how decision tree works<br><br>Decision Tree is a supervised machine learning algorithm used for classification and regression tasks which works by recursively partitioning data into subsets based on the values of input features, with the goal of predicting the target variable<br>Decision tree (Treelike structure) consist of :<br>
<br>Root Node
<br>Decision Node (Trunk)
<br>Leaf Nodes
<br><br><br>It is a field of study concerned with quantifying information for communication<br>
Entropy can be greater than 1 if you have two groups
<br><br>Gain = H - (wH + wH)<br>H, H, H = Entropy (In the group)<br>
w, w = Total data set in the group <br><br>It is a measurement for measuring the purity or the actually impurity which is used by CART algorithm called the Gini Index<br>
The impurity equal to 0 does not always mean a good thing in all cases, if a cluster only contains a single data there is no meaning for having the cluster
<br><br>
<br>Which question to ask?
<br>How much a question helps to unmix the labels? (Determine potential and best split)
<br>Quantify the amount of uncertainty at a single node using the Gini impurity metric
<br>Quantify how much a question reduces the uncertainty using Information Gain
<br>Repeating to ask questions until the data can not be divided further
<br>Right size of the tree?<br>
Like many other machine learning algorithms, decision trees are subject to potentially overfitting the training data. The solution here is pruning<br><br>The goal is to have a tree that generalize better. The idea is to cut branches that seems as a result from overfitting the training set<br>
We can use pre-pruning or post-pruning<br>Common Pruning methods :<br>
<br>Reduced Error Pruning
<br>Minimum Description Length Pruning
<br><br>Ensemble learning is a machine learning technique where multiple models are trained to solve the same problem. The predictions from the multiple models are then combined to improve overall performance<br>Commonly used methods :<br>
<br>Bagging (Bootstrapping)
<br>Boosting
<br>Stacking
<br><br>It is random sampling with replacement<br>The output of each predictor(classifier) is summarised by the aggregation function (Statistical mode for classification and average for regression) which helps to reduce bot bias and variance<br><br>Boosting aims to improve the predictive performance of a model by sequentially training multiple weak learners (typically decision trees) and focusing each subsequent learner on the mistakes made by the previous ones<br>It works by :<br>
<br>Initialization
<br>Sequential Training
<br>Weighted Voting (Combining the predictions)
<br><br>It combines multiple base models (learners) to improve predictive performance<br>It works by :<br>
<br>Base models (Multiple diverse base models are trained on a training data (Can use various base models such as, decision tree, neural networks, etc))
<br>Validation Data
<br>Collecting predictions
<br>Meta-model training
<br>Final Prediction
<br><br>Random forest is a powerful ensemble learning method for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputting the mode (for classification) or mean prediction (for regression) of the individual trees<br>When to use random forest?<br>
<br>Remote sensing by using Enhanced Theromatic Mapping devices to acquire images on the earth's surface
<br>Object detection by multiclass object detection which is done using random forest algorithms
<br>Gaming console, Random forest is used in the KINECT game console (Tracking body movements and recreates it in the game)
<br>Why Random forest?<br>
<br>No overfitting (Use of multiple trees to reduce the risk of overfitting)
<br>High accuracy 
<br>Estimates missing data
<br>How it works?<br>
<br>Bootstrap sampling
<br>Feature randomization
<br>Decision Tree Training
<br>Aggregation for predictions (Majority vote)<br>
<img alt="How Random Forest Work (1).png" src="\lib\media\how-random-forest-work-(1).png">
]]></description><link>cos10022-data-science-principles\ensemble-learning.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Ensemble Learning.md</guid><pubDate>Wed, 15 May 2024 05:26:08 GMT</pubDate><enclosure url="lib\media\how-random-forest-work-(1).png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\how-random-forest-work-(1).png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Problem Formulation]]></title><description><![CDATA[ 
 <br><br>Using proper techniques to solve the problem<br>]]></description><link>cos10022-data-science-principles\problem-formulation.html</link><guid isPermaLink="false">COS10022 Data Science Principles/Problem Formulation.md</guid><pubDate>Thu, 02 May 2024 02:26:49 GMT</pubDate></item></channel></rss>